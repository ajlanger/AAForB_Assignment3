{"title_page": "Computer simulation", "text_new": "{{About|computer model within a scientific context|simulating a computer on a computer|emulator}}\n{{redirects here|Computer model|computer models of 3 dimensional objects|3D modeling||6=|text=}}\n[[File:Molecular simulation process.svg|400px|thumb|Process of building a computer model, and the interplay between experiment, simulation, and theory.]]\n\n'''Computer simulation''' is the process of [[mathematical modelling]], performed on a [[computer]], which is designed to predict the behaviour of and/or the outcome of a real-world or physical system. Since they allow to check the reliability of chosen mathematical models, computer simulations have become a useful tool for the mathematical modeling of many natural systems in [[physics]] ([[computational physics]]), [[astrophysics]], [[climatology]], [[chemistry]], [[biology]] and [[manufacturing]], as well as human systems in [[economics]], [[psychology]], [[social science]], [[health care]] and [[engineering]]. Simulation of a system is represented as the running of the system's model. It can be used to explore and gain new insights into new [[technology]] and to estimate the performance of systems too complex for [[analytical solution]]s.<ref>{{Cite book\n  | last =Strogatz\n  | first =Steven\n  | contribution =The End of Insight\n  | year =2007\n  | title =What is your dangerous idea?\n  | editor-last =Brockman\n  | editor-first =John\n  | publisher =HarperCollins\n  | isbn=9780061214950\n  }}</ref>\n\nComputer simulations are realized by running [[computer program]]s that can be either small, running almost instantly on small devices, or large-scale programs that run for hours or days on network-based groups of computers. The scale of events being simulated by computer simulations has far exceeded anything possible (or perhaps even imaginable) using traditional paper-and-pencil mathematical modeling. Over 10 years ago, a desert-battle simulation of one force invading another involved the modeling of 66,239 tanks, trucks and other vehicles on simulated terrain around [[Kuwait]], using multiple supercomputers in the [[United States Department of Defense|DoD]] High Performance Computer Modernization Program.<ref name=\"JPLsim\">\" [http://www.jpl.nasa.gov/releases/97/military.html \"Researchers stage largest Military Simulation ever\"] {{webarchive|url=https://web.archive.org/web/20080122123958/http://www.jpl.nasa.gov/releases/97/military.html |date=2008-01-22 }}, [[Jet Propulsion Laboratory]], [[Caltech]], December 1997,</ref>\nOther examples include a 1-billion-atom model of material deformation;<ref>{{cite web|title=Molecular Simulation of Macroscopic Phenomena|url=http://www.almaden.ibm.com/st/past_projects/fractures/|url-status=live|archiveurl=https://web.archive.org/web/20130522082737/http://www.almaden.ibm.com/st/past_projects/fractures/|archivedate=2013-05-22}}</ref> a 2.64-million-atom model of the complex protein-producing organelle of all living organisms, the [[ribosome]], in 2005;<ref name=\"LANLsim\">\n   \"Largest computational biology simulation mimics life's most essential nanomachine\" (news),\n   News Release,\n   Nancy Ambrosiano, [[Los Alamos National Laboratory]],\n   Los Alamos, NM, October 2005, webpage:\n   [http://www.lanl.gov/news/index.php/fuseaction/home.story/story_id/7428 LANL-Fuse-story7428] {{webarchive|url=https://web.archive.org/web/20070704061957/http://www.lanl.gov/news/index.php/fuseaction/home.story/story_id/7428 |date=2007-07-04 }}.</ref>\na complete simulation of the life cycle of [[Mycoplasma genitalium]] in 2012; and the [[Blue Brain]] project at [[EPFL]] (Switzerland), begun in May 2005 to create the first computer simulation of the entire human brain, right down to the molecular level.<ref name=\"Brainsim\">[https://www.newscientist.com/article/dn7470.html \"Mission to build a simulated brain begins\"] {{webarchive|url=https://web.archive.org/web/20150209125048/http://www.newscientist.com/article/dn7470.html |date=2015-02-09 }}, project of the institute at the [[\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne]] (EPFL), Switzerland, ''[[New Scientist]]'', June 2005.</ref>\n\nBecause of the computational cost of simulation, [[computer experiment]]s are used to perform inference such as [[uncertainty quantification]].<ref>{{cite book| author1=Santner, Thomas J| author2=Williams, Brian J| author3=Notz, William I| title=The design and analysis of computer experiments| year=2003| publisher=Springer Verlag}}</ref>\n\n== Simulation versus model ==\nA computer model is the algorithms and equations used to capture the behavior of the system being modeled. By contrast, computer simulation is the actual running of the program that contains these equations or algorithms. Simulation, therefore, is the process of running a model. Thus one would not \"build a simulation\"; instead, one would \"build a model\", and then either \"run the model\" or equivalently \"run a simulation\".\n\n== History ==\nComputer simulation developed hand-in-hand with the rapid growth of the computer, following its first large-scale deployment during the [[Manhattan Project]] in [[World War II]] to model the process of [[nuclear weapon|nuclear detonation]]. It was a simulation of 12 [[hard spheres]] using a [[Monte Carlo method|Monte Carlo algorithm]]. Computer simulation is often used as an adjunct to, or substitute for, modeling systems for which simple [[closed-form solution|closed form analytic solutions]] are not possible. There are many types of computer simulations; their common feature is the attempt to generate a sample of representative scenarios for a model in which a complete enumeration of all possible states of the model would be prohibitive or impossible.<ref>{{Cite book|url=https://books.google.com/?id=XHnkBwAAQBAJ&pg=PR18&dq=There+are+many+types+of+computer+simulations;+their+common+feature+is+the+attempt+to+generate+a+sample+of+representative+scenarios+for+a+model+in+which+a+complete+enumeration+of+all+possible+states+of+the+model+would+be+prohibitive+or+impossible.|title=A Guide to Simulation|last=Bratley|first=Paul|last2=Fox|first2=Bennet L.|last3=Schrage|first3=Linus E.|date=2011-06-28|publisher=Springer Science & Business Media|isbn=9781441987242|language=en}}</ref>\n\n== Data preparation ==\nThe external data requirements of simulations and models vary widely. For some, the input might be just a few numbers (for example, simulation of a waveform of AC electricity on a wire), while others might require terabytes of information (such as weather and climate models).\n\nInput sources also vary widely:\n* Sensors and other physical devices connected to the model;\n* Control surfaces used to direct the progress of the simulation in some way;\n* Current or historical data entered by hand;\n* Values extracted as a by-product from other processes;\n* Values output for the purpose by other simulations, models, or processes.\n\nLastly, the time at which data is available varies:\n* \"invariant\" data is often built into the model code, either because the value is truly invariant (e.g., the value of \u03c0) or because the designers consider the value to be invariant for all cases of interest;\n* data can be entered into the simulation when it starts up, for example by reading one or more files, or by reading data from a [[preprocessor (CAE)|preprocessor]];\n* data can be provided during the simulation run, for example by a sensor network.\n\nBecause of this variety, and because diverse simulation systems have many common elements, there are a large number of specialized [[simulation language]]s. The best-known may be [[Simula]] (sometimes called Simula-67, after the year 1967 when it was proposed). There are now many others.\n\nSystems that accept data from external sources must be very careful in knowing what they are receiving. While it is easy for computers to read in values from text or binary files, what is much harder is knowing what the [[accuracy]] (compared to [[Graphic display resolutions|measurement resolution]] and [[Accuracy and precision|precision]]) of the values are. Often they are expressed as \"error bars\", a minimum and maximum deviation from the value range within which the true value (is expected to) lie. Because digital computer mathematics is not perfect, rounding and truncation errors multiply this error, so it is useful to perform an \"error analysis\"<ref name=Taylor>{{cite book |title=An Introduction to Error Analysis: The Study of Uncertainties in Physical Measurements |author=John Robert Taylor |url=https://books.google.com/books?id=giFQcZub80oC&pg=PA128 |pages=128\u2013129 |isbn=978-0-935702-75-0 |year=1999 |publisher=University Science Books |url-status=live |archiveurl=https://web.archive.org/web/20150316103343/http://books.google.com/books?id=giFQcZub80oC&pg=PA128 |archivedate=2015-03-16 }}</ref> to confirm that values output by the simulation will still be usefully accurate.\n\nEven small errors in the original data can accumulate into substantial error later in the simulation. While all computer analysis is subject to the \"GIGO\" ([[garbage in, garbage out]]) restriction, this is especially true of digital simulation. Indeed, observation of this inherent, cumulative error in digital systems was the main catalyst for the development of [[chaos theory]].\n\n== Types ==\nComputer models can be classified according to several independent pairs of attributes, including:\n* [[stochastic process|Stochastic]] or [[Deterministic algorithm|deterministic]] (and as a special case of deterministic, chaotic) \u2013 see external links below for examples of stochastic vs. deterministic simulations\n* Steady-state or dynamic\n* [[Continuous function|Continuous]] or [[discrete mathematics|discrete]] (and as an important special case of discrete, [[Discrete event simulation|discrete event]] or DE models)\n* [[Dynamic simulation|Dynamic system simulation]], e.g. electric systems, hydraulic systems or multi-body mechanical systems (described primarily by DAE:s) or dynamics simulation of field problems, e.g. CFD of FEM simulations (described by PDE:s).\n* Local or [[distributed computing|distributed]].\n\nAnother way of categorizing models is to look at the underlying data structures. For time-stepped simulations, there are two main classes:\n* Simulations which store their data in regular grids and require only next-neighbor access are called [[stencil code]]s. Many [[Computational fluid dynamics|CFD]] applications belong to this category.\n* If the underlying graph is not a regular grid, the model may belong to the [[meshfree method]] class.\n\nEquations define the relationships between elements of the modeled system and attempt to find a state in which the system is in equilibrium. Such models are often used in simulating physical systems, as a simpler modeling case before dynamic simulation is attempted.\n*Dynamic simulations model changes in a system in response to (usually changing) input signals.\n*''[[stochastic process|Stochastic]]'' models use ''[[random number generator]]s'' to model chance or random events;\n*A ''[[discrete event simulation]]'' (DES) manages events in time. Most computer, logic-test and fault-tree simulations are of this type. In this type of simulation, the simulator maintains a queue of events sorted by the simulated time they should occur. The simulator reads the queue and triggers new events as each event is processed. It is not important to execute the simulation in real time. It is often more important to be able to access the data produced by the simulation and to discover logic defects in the design or the sequence of events.\n*A ''continuous dynamic simulation'' performs numerical solution of [[Differential algebraic equation|differential-algebraic equations]] or [[differential equations]] (either [[partial differential equation|partial]] or [[ordinary differential equation|ordinary]]). Periodically, the simulation program solves all the equations and uses the numbers to change the state and output of the simulation. Applications include flight simulators, [[construction and management simulation games]], [[chemical process modeling]], and simulations of [[electrical circuit]]s. Originally, these kinds of simulations were actually implemented on [[analog computer]]s, where the differential equations could be represented directly by various electrical components such as [[op-amp]]s. By the late 1980s, however, most \"analog\" simulations were run on conventional [[digital computer]]s that [[emulator|emulate]] the behavior of an analog computer.\n*A special type of discrete simulation that does not rely on a model with an underlying equation, but can nonetheless be represented formally, is [[agent-based model|agent-based simulation]]. In agent-based simulation, the individual entities (such as molecules, cells, trees or consumers) in the model are represented directly (rather than by their density or concentration) and possess an internal state and set of behaviors or rules that determine how the agent's state is updated from one time-step to the next.\n*[[Distributed computing|Distributed]] models run on a network of interconnected computers, possibly through the [[Internet]]. Simulations dispersed across multiple host computers like this are often referred to as \"distributed simulations\". There are several standards for distributed simulation, including [[Aggregate Level Simulation Protocol]] (ALSP), [[Distributed Interactive Simulation]] (DIS), the [[High Level Architecture (simulation)]] (HLA) and the [[Test and Training Enabling Architecture]] (TENA).\n\n== Visualization ==\nFormerly, the output data from a computer simulation was sometimes presented in a table or a matrix showing how data were affected by numerous changes in the simulation [[Parameter (computer programming)|parameters]]. The use of the matrix format was related to traditional use of the matrix concept in [[mathematical model]]s. However, psychologists and others noted that humans could quickly perceive trends by looking at graphs or even moving-images or motion-pictures generated from the data, as displayed by [[computer generated imagery|computer-generated-imagery]] (CGI) animation. Although observers could not necessarily read out numbers or quote math formulas, from observing a moving weather chart they might be able to predict events (and \"see that rain was headed their way\") much faster than by scanning tables of rain-cloud [[coordinate]]s. Such intense graphical displays, which transcended the world of numbers and formulae, sometimes also led to output that lacked a coordinate grid or omitted timestamps, as if straying too far from numeric data displays. Today, [[weather forecasting]] models tend to balance the view of moving rain/snow clouds against a map that uses numeric coordinates and numeric timestamps of events.\n\nSimilarly, CGI computer simulations of [[CAT scan]]s can simulate how a [[brain cancer|tumor]] might shrink or change during an extended period of medical treatment, presenting the passage of time as a spinning view of the visible human head, as the tumor changes.\n\nOther applications of CGI computer simulations are being developed to graphically display large amounts of data, in motion, as changes occur during a simulation run.\n\n== Computer simulation in science ==\n[[File:Osmosis computer simulation.jpg|250px|thumb|Computer simulation of the process of [[osmosis]] ]]\n\nGeneric examples of types of computer simulations in science, which are derived from an underlying mathematical description:\n* a numerical simulation of [[differential equation]]s that cannot be solved analytically, theories that involve continuous systems such as phenomena in [[physical cosmology]], [[fluid dynamics]] (e.g., [[climate model]]s, [[roadway noise]] models, [[roadway air dispersion model]]s), [[continuum mechanics]] and [[chemical kinetics]] fall into this category.\n* a [[stochastic]] simulation, typically used for discrete systems where events occur [[probabilistic]]ally and which cannot be described directly with differential equations (this is a ''discrete'' simulation in the above sense). Phenomena in this category include [[genetic drift]], [[biochemistry|biochemical]]<ref name=\":0\">{{Cite journal|last=Gupta|first=Ankur|last2=Rawlings|first2=James B.|date=April 2014|title=Comparison of Parameter Estimation Methods in Stochastic Chemical Kinetic Models: Examples in Systems Biology|journal=AIChE Journal |volume=60|issue=4|pages=1253\u20131268|doi=10.1002/aic.14409|issn=0001-1541|pmc=4946376|pmid=27429455}}</ref> or [[gene regulatory network]]s with small numbers of molecules. (see also: [[Monte Carlo method]]).\n* multiparticle simulation of the response of nanomaterials at multiple scales to an applied force for the purpose of modeling their thermoelastic and thermodynamic properties. Techniques used for such simulations are [[Molecular dynamics]], [[Molecular mechanics]], [[Monte Carlo method]], and [[Multiscale Green's function]].\n\nSpecific examples of computer simulations follow:\n* statistical simulations based upon an agglomeration of a large number of input profiles, such as the forecasting of equilibrium [[temperature]] of [[receiving waters]], allowing the gamut of [[meteorological]] data to be input for a specific locale. This technique was developed for [[thermal pollution]] forecasting.\n* agent based simulation has been used effectively in [[ecology]], where it is often called \"individual based modeling\" and is used in situations for which individual variability in the agents cannot be neglected, such as [[population dynamics]] of [[salmon]] and [[trout]] (most purely mathematical models assume all trout behave identically).\n* time stepped dynamic model. In hydrology there are several such [[hydrology transport model]]s such as the [[SWMM]] and [[DSSAM Model]]s developed by the [[United States Environmental Protection Agency|U.S. Environmental Protection Agency]] for river water quality forecasting.\n* computer simulations have also been used to formally model theories of human cognition and performance, e.g., [[ACT-R]].\n* computer simulation using [[molecular modeling]] for [[drug discovery]].<ref>Atanasov AG, Waltenberger B, Pferschy-Wenzig EM, Linder T, Wawrosch C, Uhrin P, Temml V, Wang L, Schwaiger S, Heiss EH, Rollinger JM, Schuster D, Breuss JM, Bochkov V, Mihovilovic MD, Kopp B, Bauer R, Dirsch VM, Stuppner H. {{doi|10.1016/j.biotechadv.2015.08.001}} Discovery and resupply of pharmacologically active plant-derived natural products: A review.] Biotechnol Adv. 2015, {{PMID|26281720}}.</ref>\n*computer simulation to model viral infection in mammalian cells.<ref name=\":0\" />\n* computer simulation for studying the selective sensitivity of bonds by mechanochemistry during grinding of organic molecules.<ref>Mizukami, Koichi ; Saito, Fumio ; Baron, Michel. [http://pem.utbm.fr/materiaux_2002/file/pdf/AF01078.PDF Study on grinding of pharmaceutical products with an aid of computer simulation] {{webarchive|url=https://web.archive.org/web/20110721023918/http://pem.utbm.fr/materiaux_2002/file/pdf/AF01078.PDF |date=2011-07-21 }}</ref>\n* [[Computational fluid dynamics]] simulations are used to simulate the behaviour of flowing air, water and other fluids. One-, two- and three-dimensional models are used. A one-dimensional model might simulate the effects of [[water hammer]] in a pipe. A two-dimensional model might be used to simulate the drag forces on the cross-section of an aeroplane wing. A three-dimensional simulation might estimate the heating and cooling requirements of a large building.\n* An understanding of statistical thermodynamic molecular theory is fundamental to the appreciation of molecular solutions. Development of the [[Potential Distribution Theorem]] (PDT) allows this complex subject to be simplified to down-to-earth presentations of molecular theory.\n\nNotable, and sometimes controversial, computer simulations used in science include: [[Donella Meadows]]' [[World3]] used in the ''[[Limits to Growth]]'', [[James Lovelock|James Lovelock's]] [[Daisyworld]] and Thomas Ray's [[Tierra (computer simulation)|Tierra]].\n\nIn social sciences, computer simulation is an integral component of the five angles of analysis fostered by the data percolation methodology,<ref>Mesly, Olivier \n(2015). ''Creating Models in Psychological Research.'' United States: Springer Psychology: 126 pages. {{ISBN|978-3-319-15752-8}}</ref> which also includes qualitative and quantitative methods, reviews of the literature (including scholarly), and interviews with experts, and which forms an extension of data triangulation.\n\n=== Simulation environments for physics and engineering ===\n[[Graphical environment]]s to design simulations have been developed. Special care was taken to handle events (situations in which the simulation equations are not valid and have to be changed). The open project [[Open Source Physics]] was started to develop reusable libraries for simulations in [[Java (programming language)|Java]], together with [[EJS|Easy Java Simulations]], a complete graphical environment that generates code based on these libraries.\n\n=== Simulation environments for linguistics ===\nTaiwanese Tone Group Parser<ref>{{cite journal | last1 = Chang | first1 = Y. C. | year = 2017 | title = A Knowledge Representation Method to Implement A Taiwanese Tone Group Parser [In Chinese] | url = | journal = International Journal of Computational Linguistics & Chinese Language Processing | volume = 22 | issue = 212| pages = 73\u201386 }}</ref> is a simulator of Taiwanese tone sandhi acquisition.\nIn practical, the method using linguistic theory to implement the Taiwanese tone group parser is a way to apply [[knowledge engineering]] technique to build the experiment environment of computer simulation for language acquisition. A work-in-process version of artificial tone group parser that includes a [[knowledge base]] and an executable program file for Microsoft Windows system (XP/Win7) can be [http://vikon.myweb.hinet.net/ttgpe.htm download] for evaluation.\n\n== Computer simulation in practical contexts ==\nComputer simulations are used in a wide variety of practical contexts, such as:\n* analysis of [[air pollutant]] dispersion using [[atmospheric dispersion modeling]]\n* design of complex systems such as [[aircraft]] and also [[logistics]] systems.\n* design of [[noise barrier]]s to effect roadway [[noise mitigation]]\n* modeling of [[Application performance management|application performance]]<ref>{{cite book | last = Wescott | first = Bob | title = The Every Computer Performance Book, Chapter 7: Modeling Computer Performance | publisher = [[CreateSpace]] | date = 2013 | isbn = 978-1482657753 | url = https://books.google.com/books?id=0SD1mgEACAAJ}}</ref>\n* [[flight simulator]]s to train pilots\n* [[Atmospheric model|weather forecasting]]\n* [[risk management|forecasting of risk]]\n* simulation of electrical circuits\n* [[Power system simulation]]\n* simulation of other computers is [[Emulator|emulation]].\n* forecasting of prices on financial markets (for example [[Adaptive Modeler]])\n* behavior of structures (such as buildings and industrial parts) under stress and other conditions\n* design of industrial processes, such as chemical processing plants\n* [[strategic management]] and [[organizational studies]]\n* [[reservoir simulation]] for the petroleum engineering to model the subsurface reservoir\n* process engineering simulation tools.\n* [[Robotics suite|robot simulators]] for the design of robots and robot control algorithms\n* [[UrbanSim|urban simulation models]] that simulate dynamic patterns of urban development and responses to urban land use and transportation policies. See a more detailed article on [[Urban Environment Simulation]].\n* [[Traffic engineering (transportation)|traffic engineering]] to plan or redesign parts of the street network from single junctions over cities to a national highway network to transportation system planning, design and operations. See a more detailed article on [[Traffic Simulation|Simulation in Transportation]].\n* modeling car crashes to test safety mechanisms in new vehicle models.\n* [[Theoretical production ecology|crop-soil systems]] in agriculture, via dedicated software frameworks (e.g. [[Biophysical Models|BioMA]], OMS3, APSIM)\n\nThe reliability and the trust people put in computer simulations depends on the [[Validity (logic)|validity]] of the simulation [[model (abstract)|model]], therefore [[verification and validation]] are of crucial importance in the development of computer simulations. Another important aspect of computer simulations is that of reproducibility of the results, meaning that a simulation model should not provide a different answer for each execution. Although this might seem obvious, this is a special point of attention in [[stochastic simulation]]s, where random numbers should actually be semi-random numbers. An exception to reproducibility are human-in-the-loop simulations such as flight simulations and [[computer games]]. Here a human is part of the simulation and thus influences the outcome in a way that is hard, if not impossible, to reproduce exactly.\n\n[[Vehicle]] manufacturers make use of computer simulation to test safety features in new designs. By building a copy of the car in a physics simulation environment, they can save the hundreds of thousands of dollars that would otherwise be required to build and test a unique prototype. Engineers can step through the simulation milliseconds at a time to determine the exact stresses being put upon each section of the prototype.<ref>Baase, Sara. A Gift of Fire: Social, Legal, and Ethical Issues for Computing and the Internet. 3. Upper Saddle River: Prentice Hall, 2007. Pages 363\u2013364. {{ISBN|0-13-600848-8}}.</ref>\n\n[[Computer graphics]] can be used to display the results of a computer simulation. [[Animations]] can be used to experience a simulation in real-time, e.g., in [[Training Simulation|training simulations]]. In some cases animations may also be useful in faster than real-time or even slower than real-time modes. For example, faster than real-time animations can be useful in visualizing the buildup of queues in the simulation of humans evacuating a building. Furthermore, simulation results are often aggregated into static images using various ways of [[scientific visualization]].\n\nIn debugging, simulating a program execution under test (rather than executing natively) can detect far more errors than the hardware itself can detect and, at the same time, log useful debugging information such as instruction trace, memory alterations and instruction counts. This technique can also detect [[buffer overflow]] and similar \"hard to detect\" errors as well as produce performance information and [[Performance tuning|tuning]] data.\n\n== Pitfalls ==\nAlthough sometimes ignored in computer simulations, it is very important to perform a [[sensitivity analysis]] to ensure that the accuracy of the results is properly understood. For example, the probabilistic risk analysis of factors determining the success of an oilfield exploration program involves combining samples from a variety of statistical distributions using the [[Monte Carlo method]]. If, for instance, one of the key parameters (e.g., the net ratio of oil-bearing strata) is known to only one significant figure, then the result of the simulation might not be more precise than one significant figure, although it might (misleadingly) be presented as having four significant figures.\n\n=== Model calibration techniques ===\nThe following three steps should be used to produce accurate simulation models: calibration, verification, and validation. Computer simulations are good at portraying and comparing theoretical scenarios, but in order to accurately model actual case studies they have to match what is actually happening today. A base model should be created and calibrated so that it matches the area being studied. The calibrated model should then be verified to ensure that the model is operating as expected based on the inputs. Once the model has been verified, the final step is to validate the model by comparing the outputs to historical data from the study area. This can be done by using statistical techniques and ensuring an adequate R-squared value. Unless these techniques are employed, the simulation model created will produce inaccurate results and not be a useful prediction tool.\n\nModel calibration is achieved by adjusting any available parameters in order to adjust how the model operates and simulates the process. For example, in traffic simulation, typical parameters include look-ahead distance, car-following sensitivity, discharge headway, and start-up lost time. These parameters influence driver behavior such as when and how long it takes a driver to change lanes, how much distance a driver leaves between his car and the car in front of it, and how quickly a driver starts to accelerate through an intersection. Adjusting these parameters has a direct effect on the amount of traffic volume that can traverse through the modeled roadway network by making the drivers more or less aggressive. These are examples of calibration parameters that can be fine-tuned to match characteristics observed in the field at the study location. Most traffic models have typical default values but they may need to be adjusted to better match the driver behavior at the specific location being studied.\n\nModel verification is achieved by obtaining output data from the model and comparing them to what is expected from the input data. For example, in traffic simulation, traffic volume can be verified to ensure that actual volume throughput in the model is reasonably close to traffic volumes input into the model. Ten percent is a typical threshold used in traffic simulation to determine if output volumes are reasonably close to input volumes. Simulation models handle model inputs in different ways so traffic that enters the network, for example, may or may not reach its desired destination. Additionally, traffic that wants to enter the network may not be able to, if congestion exists. This is why model verification is a very important part of the modeling process.\n\nThe final step is to validate the model by comparing the results with what is expected based on historical data from the study area. Ideally, the model should produce similar results to what has happened historically. This is typically verified by nothing more than quoting the R-squared statistic from the fit. This statistic measures the fraction of variability that is accounted for by the model. A high R-squared value does not necessarily mean the model fits the data well. Another tool used to validate models is graphical residual analysis. If model output values drastically differ from historical values, it probably means there is an error in the model. Before using the model as a base to produce additional models, it is important to verify it for different scenarios to ensure that each one is accurate. If the outputs do not reasonably match historic values during the validation process, the model should be reviewed and updated to produce results more in line with expectations. It is an iterative process that helps to produce more realistic models.\n\nValidating traffic simulation models requires comparing traffic estimated by the model to observed traffic on the roadway and transit systems. Initial comparisons are for trip interchanges between quadrants, sectors, or other large areas of interest. The next step is to compare traffic estimated by the models to traffic counts, including transit ridership, crossing contrived barriers in the study area. These are typically called screenlines, cutlines, and cordon lines and may be imaginary or actual physical barriers. Cordon lines surround particular areas such as a city's central business district or other major activity centers. Transit ridership estimates are commonly validated by comparing them to actual patronage crossing cordon lines around the central business district.\n\nThree sources of error can cause weak correlation during calibration: input error, model error, and parameter error. In general, input error and parameter error can be adjusted easily by the user. Model error however is caused by the methodology used in the model and may not be as easy to fix. Simulation models are typically built using several different modeling theories that can produce conflicting results. Some models are more generalized while others are more detailed. If model error occurs as a result, in may be necessary to adjust the model methodology to make results more consistent.\n\nIn order to produce good models that can be used to produce realistic results, these are the necessary steps that need to be taken in order to ensure that simulation models are functioning properly. Simulation models can be used as a tool to verify engineering theories, but they are only valid if calibrated properly. Once satisfactory estimates of the parameters for all models have been obtained, the models must be checked to assure that they adequately perform the intended functions. The validation process establishes the credibility of the model by demonstrating its ability to replicate reality. The importance of model validation underscores the need for careful planning, thoroughness and accuracy of the input data collection program that has this purpose. Efforts should be made to ensure collected data is consistent with expected values. For example, in traffic analysis it is typical for a traffic engineer to perform a site visit to verify traffic counts and become familiar with traffic patterns in the area. The resulting models and forecasts will be no better than the data used for model estimation and validation.\n\n== See also ==\n\n[[File:Typhoon Mawar 2005 computer simulation thumbnail.gif|300px|thumb|A 48-hour computer simulation of [[2005 Pacific typhoon season#Typhoon Mawar|Typhoon Mawar]] using the [[Weather Research and Forecasting model]] ]]\n{{div col|colwidth=30em}}\n* [[Computational model]]\n* [[Emulator]]\n* [[Energy modeling]]\n* [[Illustris project]]\n* [[List of computer simulation software]]\n* [[Stencil code]]\n* [[UniverseMachine]]\n* [[Virtual prototyping]]\n* [[Web-based simulation]]\n{{div col end}}\n\n== References ==\n{{More footnotes|date=May 2008}}\n{{Reflist}}\n\n== Further reading ==\n{{Commons category}}\n* {{cite journal | last1 = Kafashan | first1 = J. | last2 = Wi\u0105cek | first2 = J. | last3 = Abd Rahman | first3 = N. | last4 = Gan | first4 = J. | year = 2019 | title = Two-dimensional particle shapes modelling for DEM simulations in engineering: a review | url = https://link.springer.com/article/10.1007/s10035-019-0935-1 | journal = Granular Matter | volume = 21 | issue = | page = 80 | doi = 10.1007/s10035-019-0935-1 }}\n*[https://www.taylorfrancis.com/books/9781351241120/ \"Modeling and Simulation\"], G. Dubois, Taylor & Francis, CRC Press, 2018.\n* [http://www.cuideas.org/publications/ \"A Resource Allocation Framework for Experiment-Based Validation of Numerical Models,\"] Journal of Mechanics of Advanced Materials and Structures (Taylor & Francis).\n* Young, Joseph and Findley, Michael. 2014. \"Computational Modeling to Study Conflicts and Terrorism.\" [https://books.google.com/books?id=ENDpAwAAQBAJ&pg=PT23 Routledge Handbook of Research Methods in Military Studies] edited by Soeters, Joseph; Shields, Patricia and Rietjens, Sebastiaan. pp.&nbsp;249\u2013260. New York: Routledge,\n* R. Frigg and S. Hartmann, [http://plato.stanford.edu/entries/models-science/ Models in Science]. Entry in the '' [[Stanford Encyclopedia of Philosophy]]''.\n*E. Winsberg [http://plato.stanford.edu/entries/simulations-science/ Simulation in Science]. Entry in the '' [[Stanford Encyclopedia of Philosophy]]''.\n* A.K. Hartmann, [https://web.archive.org/web/20090211113048/http://worldscibooks.com/physics/6988.html Practical Guide to Computer Simulations], Singapore: [[World Scientific]], 2009\n* S. Hartmann, [http://philsci-archive.pitt.edu/archive/00002412/ The World as a Process: Simulations in the Natural and Social Sciences], in: R. Hegselmann et al. (eds.), ''Modelling and Simulation in the Social Sciences from the Philosophy of Science Point of View'', Theory and Decision Library. Dordrecht: [[Kluwer]] 1996, 77\u2013100.\n* E. Winsberg, ''Science in the Age of Computer Simulation''. Chicago: [[University of Chicago Press]], 2010.\n* P. Humphreys, ''Extending Ourselves: Computational Science, Empiricism, and Scientific Method''. Oxford: [[Oxford University Press]], 2004.\n* {{cite book|author=James J. Nutaro|title=Building Software for Simulation: Theory and Algorithms, with Applications in C++|url=https://books.google.com/books?id=WZceCd74GRcC|year=2011|publisher=John Wiley & Sons|isbn=978-1-118-09945-2}}\n* Desa, W. L. H. M., Kamaruddin, S., & Nawawi, M. K. M. (2012). Modeling of Aircraft Composite Parts Using Simulation. Advanced Material Research, 591\u2013593, 557\u2013560.\n\n{{Computer simulation}}\n{{Energy modeling}}\n{{Authority control}}\n\n{{DEFAULTSORT:Computer Simulation}}\n[[Category:Computational science]]\n[[Category:Scientific modeling]]\n[[Category:Simulation software| ]]\n[[Category:Virtual reality]]\n[[Category:Alternatives to animal testing]]\n[[Category:Computational fields of study]]\n", "text_old": "{{About|computer model within a scientific context|simulating a computer on a computer|emulator}}\n{{redirects here|Computer model|computer models of 3 dimensional objects|3D modeling||6=|text=}}\n[[File:Molecular simulation process.svg|400px|thumb|Process of building a computer model, and the interplay between experiment, simulation, and theory.]]\n\n'''Computer simulation''' is the process of [[mathematical modelling]], performed on a [[computer]], which is designed to predict the behaviour of and/or the outcome of a real-world or physical system. Since they allow to check the reliability of chosen mathematical models, computer simulations have become a useful tool for the mathematical modeling of many natural systems in [[physics]] ([[computational physics]]), [[astrophysics]], [[climatology]], [[chemistry]], [[biology]] and [[manufacturing]], as well as human systems in [[economics]], [[psychology]], [[social science]], [[health care]] and [[engineering]]. Simulation of a system is represented as the running of the system's model. It can be used to explore and gain new insights into new [[technology]] and to estimate the performance of systems too complex for [[analytical solution]]s.<ref>{{Cite book\n  | last =Strogatz\n  | first =Steven\n  | contribution =The End of Insight\n  | year =2007\n  | title =What is your dangerous idea?\n  | editor-last =Brockman\n  | editor-first =John\n  | publisher =HarperCollins\n  | isbn=9780061214950\n  }}</ref>\n\nComputer simulations are realized by running [[computer program]]s that can be either small, running almost instantly on small devices, or large-scale programs that run for hours or days on network-based groups of computers. The scale of events being simulated by computer simulations has far exceeded anything possible (or perhaps even imaginable) using traditional paper-and-pencil mathematical modeling. Over 10 years ago, a desert-battle simulation of one force invading another involved the modeling of 66,239 tanks, trucks and other vehicles on simulated terrain around [[Kuwait]], using multiple supercomputers in the [[United States Department of Defense|DoD]] High Performance Computer Modernization Program.<ref name=\"JPLsim\">\" [http://www.jpl.nasa.gov/releases/97/military.html \"Researchers stage largest Military Simulation ever\"] {{webarchive|url=https://web.archive.org/web/20080122123958/http://www.jpl.nasa.gov/releases/97/military.html |date=2008-01-22 }}, [[Jet Propulsion Laboratory]], [[Caltech]], December 1997,</ref>\nOther examples include a 1-billion-atom model of material deformation;<ref>{{cite web|title=Molecular Simulation of Macroscopic Phenomena|url=http://www.almaden.ibm.com/st/past_projects/fractures/|url-status=live|archiveurl=https://web.archive.org/web/20130522082737/http://www.almaden.ibm.com/st/past_projects/fractures/|archivedate=2013-05-22}}</ref> a 2.64-million-atom model of the complex protein-producing organelle of all living organisms, the [[ribosome]], in 2005;<ref name=\"LANLsim\">\n   \"Largest computational biology simulation mimics life's most essential nanomachine\" (news),\n   News Release,\n   Nancy Ambrosiano, [[Los Alamos National Laboratory]],\n   Los Alamos, NM, October 2005, webpage:\n   [http://www.lanl.gov/news/index.php/fuseaction/home.story/story_id/7428 LANL-Fuse-story7428] {{webarchive|url=https://web.archive.org/web/20070704061957/http://www.lanl.gov/news/index.php/fuseaction/home.story/story_id/7428 |date=2007-07-04 }}.</ref>\na complete simulation of the life cycle of [[Mycoplasma genitalium]] in 2012; and the [[Blue Brain]] project at [[EPFL]] (Switzerland), begun in May 2005 to create the first computer simulation of the entire human brain, right down to the molecular level.<ref name=\"Brainsim\">[https://www.newscientist.com/article/dn7470.html \"Mission to build a simulated brain begins\"] {{webarchive|url=https://web.archive.org/web/20150209125048/http://www.newscientist.com/article/dn7470.html |date=2015-02-09 }}, project of the institute at the [[\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne]] (EPFL), Switzerland, ''[[New Scientist]]'', June 2005.</ref>\n\nBecause of the computational cost of simulation, [[computer experiment]]s are used to perform inference such as [[uncertainty quantification]].<ref>{{cite book| author1=Santner, Thomas J| author2=Williams, Brian J| author3=Notz, William I| title=The design and analysis of computer experiments| year=2003| publisher=Springer Verlag}}</ref>\n\n== Simulation versus model ==\nA computer model is the algorithms and equations used to capture the behavior of the system being modeled. By contrast, computer simulation is the actual running of the program that contains these equations or algorithms. Simulation, therefore, is the process of running a model. Thus one would not \"build a simulation\"; instead, one would \"build a model\", and then either \"run the model\" or equivalently \"run a simulation\".\n\n== History ==\nComputer simulation developed hand-in-hand with the rapid growth of the computer, following its first large-scale deployment during the [[Manhattan Project]] in [[World War II]] to model the process of [[nuclear weapon|nuclear detonation]]. It was a simulation of 12 [[hard spheres]] using a [[Monte Carlo method|Monte Carlo algorithm]]. Computer simulation is often used as an adjunct to, or substitute for, modeling systems for which simple [[closed-form solution|closed form analytic solutions]] are not possible. There are many types of computer simulations; their common feature is the attempt to generate a sample of representative scenarios for a model in which a complete enumeration of all possible states of the model would be prohibitive or impossible.<ref>{{Cite book|url=https://books.google.com/?id=XHnkBwAAQBAJ&pg=PR18&dq=There+are+many+types+of+computer+simulations;+their+common+feature+is+the+attempt+to+generate+a+sample+of+representative+scenarios+for+a+model+in+which+a+complete+enumeration+of+all+possible+states+of+the+model+would+be+prohibitive+or+impossible.|title=A Guide to Simulation|last=Bratley|first=Paul|last2=Fox|first2=Bennet L.|last3=Schrage|first3=Linus E.|date=2011-06-28|publisher=Springer Science & Business Media|isbn=9781441987242|language=en}}</ref>\n\n== Data preparation ==\nThe external data requirements of simulations and models vary widely. For some, the input might be just a few numbers (for example, simulation of a waveform of AC electricity on a wire), while others might require terabytes of information (such as weather and climate models).\n\nInput sources also vary widely:\n* Sensors and other physical devices connected to the model;\n* Control surfaces used to direct the progress of the simulation in some way;\n* Current or historical data entered by hand;\n* Values extracted as a by-product from other processes;\n* Values output for the purpose by other simulations, models, or processes.\n\nLastly, the time at which data is available varies:\n* \"invariant\" data is often built into the model code, either because the value is truly invariant (e.g., the value of \u03c0) or because the designers consider the value to be invariant for all cases of interest;\n* data can be entered into the simulation when it starts up, for example by reading one or more files, or by reading data from a [[preprocessor (CAE)|preprocessor]];\n* data can be provided during the simulation run, for example by a sensor network.\n\nBecause of this variety, and because diverse simulation systems have many common elements, there are a large number of specialized [[simulation language]]s. The best-known may be [[Simula]] (sometimes called Simula-67, after the year 1967 when it was proposed). There are now many others.\n\nSystems that accept data from external sources must be very careful in knowing what they are receiving. While it is easy for computers to read in values from text or binary files, what is much harder is knowing what the [[accuracy]] (compared to [[Graphic display resolutions|measurement resolution]] and [[Accuracy and precision|precision]]) of the values are. Often they are expressed as \"error bars\", a minimum and maximum deviation from the value range within which the true value (is expected to) lie. Because digital computer mathematics is not perfect, rounding and truncation errors multiply this error, so it is useful to perform an \"error analysis\"<ref name=Taylor>{{cite book |title=An Introduction to Error Analysis: The Study of Uncertainties in Physical Measurements |author=John Robert Taylor |url=https://books.google.com/books?id=giFQcZub80oC&pg=PA128 |pages=128\u2013129 |isbn=978-0-935702-75-0 |year=1999 |publisher=University Science Books |url-status=live |archiveurl=https://web.archive.org/web/20150316103343/http://books.google.com/books?id=giFQcZub80oC&pg=PA128 |archivedate=2015-03-16 }}</ref> to confirm that values output by the simulation will still be usefully accurate.\n\nEven small errors in the original data can accumulate into substantial error later in the simulation. While all computer analysis is subject to the \"GIGO\" ([[garbage in, garbage out]]) restriction, this is especially true of digital simulation. Indeed, observation of this inherent, cumulative error in digital systems was the main catalyst for the development of [[chaos theory]].\n\n== Types ==\nComputer models can be classified according to several independent pairs of attributes, including:\n* [[stochastic process|Stochastic]] or [[Deterministic algorithm|deterministic]] (and as a special case of deterministic, chaotic) \u2013 see external links below for examples of stochastic vs. deterministic simulations\n* Steady-state or dynamic\n* [[Continuous function|Continuous]] or [[discrete mathematics|discrete]] (and as an important special case of discrete, [[Discrete event simulation|discrete event]] or DE models)\n* [[Dynamic simulation|Dynamic system simulation]], e.g. electric systems, hydraulic systems or multi-body mechanical systems (described primarily by DAE:s) or dynamics simulation of field problems, e.g. CFD of FEM simulations (described by PDE:s).\n* Local or [[distributed computing|distributed]].\n\nAnother way of categorizing models is to look at the underlying data structures. For time-stepped simulations, there are two main classes:\n* Simulations which store their data in regular grids and require only next-neighbor access are called [[stencil code]]s. Many [[Computational fluid dynamics|CFD]] applications belong to this category.\n* If the underlying graph is not a regular grid, the model may belong to the [[meshfree method]] class.\n\nEquations define the relationships between elements of the modeled system and attempt to find a state in which the system is in equilibrium. Such models are often used in simulating physical systems, as a simpler modeling case before dynamic simulation is attempted.\n*Dynamic simulations model changes in a system in response to (usually changing) input signals.\n*''[[stochastic process|Stochastic]]'' models use ''[[random number generator]]s'' to model chance or random events;\n*A ''[[discrete event simulation]]'' (DES) manages events in time. Most computer, logic-test and fault-tree simulations are of this type. In this type of simulation, the simulator maintains a queue of events sorted by the simulated time they should occur. The simulator reads the queue and triggers new events as each event is processed. It is not important to execute the simulation in real time. It is often more important to be able to access the data produced by the simulation and to discover logic defects in the design or the sequence of events.\n*A ''continuous dynamic simulation'' performs numerical solution of [[Differential algebraic equation|differential-algebraic equations]] or [[differential equations]] (either [[partial differential equation|partial]] or [[ordinary differential equation|ordinary]]). Periodically, the simulation program solves all the equations and uses the numbers to change the state and output of the simulation. Applications include flight simulators, [[construction and management simulation games]], [[chemical process modeling]], and simulations of [[electrical circuit]]s. Originally, these kinds of simulations were actually implemented on [[analog computer]]s, where the differential equations could be represented directly by various electrical components such as [[op-amp]]s. By the late 1980s, however, most \"analog\" simulations were run on conventional [[digital computer]]s that [[emulator|emulate]] the behavior of an analog computer.\n*A special type of discrete simulation that does not rely on a model with an underlying equation, but can nonetheless be represented formally, is [[agent-based model|agent-based simulation]]. In agent-based simulation, the individual entities (such as molecules, cells, trees or consumers) in the model are represented directly (rather than by their density or concentration) and possess an internal state and set of behaviors or rules that determine how the agent's state is updated from one time-step to the next.\n*[[Distributed computing|Distributed]] models run on a network of interconnected computers, possibly through the [[Internet]]. Simulations dispersed across multiple host computers like this are often referred to as \"distributed simulations\". There are several standards for distributed simulation, including [[Aggregate Level Simulation Protocol]] (ALSP), [[Distributed Interactive Simulation]] (DIS), the [[High Level Architecture (simulation)]] (HLA) and the [[Test and Training Enabling Architecture]] (TENA).\n\n== Visualization ==\nFormerly, the output data from a computer simulation was sometimes presented in a table or a matrix showing how data were affected by numerous changes in the simulation [[Parameter (computer programming)|parameters]]. The use of the matrix format was related to traditional use of the matrix concept in [[mathematical model]]s. However, psychologists and others noted that humans could quickly perceive trends by looking at graphs or even moving-images or motion-pictures generated from the data, as displayed by [[computer generated imagery|computer-generated-imagery]] (CGI) animation. Although observers could not necessarily read out numbers or quote math formulas, from observing a moving weather chart they might be able to predict events (and \"see that rain was headed their way\") much faster than by scanning tables of rain-cloud [[coordinate]]s. Such intense graphical displays, which transcended the world of numbers and formulae, sometimes also led to output that lacked a coordinate grid or omitted timestamps, as if straying too far from numeric data displays. Today, [[weather forecasting]] models tend to balance the view of moving rain/snow clouds against a map that uses numeric coordinates and numeric timestamps of events.\n\nSimilarly, CGI computer simulations of [[CAT scan]]s can simulate how a [[brain cancer|tumor]] might shrink or change during an extended period of medical treatment, presenting the passage of time as a spinning view of the visible human head, as the tumor changes.\n\nOther applications of CGI computer simulations are being developed to graphically display large amounts of data, in motion, as changes occur during a simulation run.\n\n== Computer simulation in science ==\n[[File:Osmosis computer simulation.jpg|250px|thumb|Computer simulation of the process of [[osmosis]] ]]\n\nGeneric examples of types of computer simulations in science, which are derived from an underlying mathematical description:\n* a numerical simulation of [[differential equation]]s that cannot be solved analytically, theories that involve continuous systems such as phenomena in [[physical cosmology]], [[fluid dynamics]] (e.g., [[climate model]]s, [[roadway noise]] models, [[roadway air dispersion model]]s), [[continuum mechanics]] and [[chemical kinetics]] fall into this category.\n* a [[stochastic]] simulation, typically used for discrete systems where events occur [[probabilistic]]ally and which cannot be described directly with differential equations (this is a ''discrete'' simulation in the above sense). Phenomena in this category include [[genetic drift]], [[biochemistry|biochemical]]<ref name=\":0\">{{Cite journal|last=Gupta|first=Ankur|last2=Rawlings|first2=James B.|date=April 2014|title=Comparison of Parameter Estimation Methods in Stochastic Chemical Kinetic Models: Examples in Systems Biology|journal=AIChE Journal |volume=60|issue=4|pages=1253\u20131268|doi=10.1002/aic.14409|issn=0001-1541|pmc=4946376|pmid=27429455}}</ref> or [[gene regulatory network]]s with small numbers of molecules. (see also: [[Monte Carlo method]]).\n* multiparticle simulation of the response of nanomaterials at multiple scales to an applied force for the purpose of modeling their thermoelastic and thermodynamic properties. Techniques used for such simulations are [[Molecular dynamics]], [[Molecular mechanics]], [[Monte Carlo method]], and [[Multiscale Green's function]].\n\nSpecific examples of computer simulations follow:\n* statistical simulations based upon an agglomeration of a large number of input profiles, such as the forecasting of equilibrium [[temperature]] of [[receiving waters]], allowing the gamut of [[meteorological]] data to be input for a specific locale. This technique was developed for [[thermal pollution]] forecasting.\n* agent based simulation has been used effectively in [[ecology]], where it is often called \"individual based modeling\" and is used in situations for which individual variability in the agents cannot be neglected, such as [[population dynamics]] of [[salmon]] and [[trout]] (most purely mathematical models assume all trout behave identically).\n* time stepped dynamic model. In hydrology there are several such [[hydrology transport model]]s such as the [[SWMM]] and [[DSSAM Model]]s developed by the [[United States Environmental Protection Agency|U.S. Environmental Protection Agency]] for river water quality forecasting.\n* computer simulations have also been used to formally model theories of human cognition and performance, e.g., [[ACT-R]].\n* computer simulation using [[molecular modeling]] for [[drug discovery]].<ref>Atanasov AG, Waltenberger B, Pferschy-Wenzig EM, Linder T, Wawrosch C, Uhrin P, Temml V, Wang L, Schwaiger S, Heiss EH, Rollinger JM, Schuster D, Breuss JM, Bochkov V, Mihovilovic MD, Kopp B, Bauer R, Dirsch VM, Stuppner H. {{doi|10.1016/j.biotechadv.2015.08.001}} Discovery and resupply of pharmacologically active plant-derived natural products: A review.] Biotechnol Adv. 2015, {{PMID|26281720}}.</ref>\n*computer simulation to model viral infection in mammalian cells.<ref name=\":0\" />\n* computer simulation for studying the selective sensitivity of bonds by mechanochemistry during grinding of organic molecules.<ref>Mizukami, Koichi ; Saito, Fumio ; Baron, Michel. [http://pem.utbm.fr/materiaux_2002/file/pdf/AF01078.PDF Study on grinding of pharmaceutical products with an aid of computer simulation] {{webarchive|url=https://web.archive.org/web/20110721023918/http://pem.utbm.fr/materiaux_2002/file/pdf/AF01078.PDF |date=2011-07-21 }}</ref>\n* [[Computational fluid dynamics]] simulations are used to simulate the behaviour of flowing air, water and other fluids. One-, two- and three-dimensional models are used. A one-dimensional model might simulate the effects of [[water hammer]] in a pipe. A two-dimensional model might be used to simulate the drag forces on the cross-section of an aeroplane wing. A three-dimensional simulation might estimate the heating and cooling requirements of a large building.\n* An understanding of statistical thermodynamic molecular theory is fundamental to the appreciation of molecular solutions. Development of the [[Potential Distribution Theorem]] (PDT) allows this complex subject to be simplified to down-to-earth presentations of molecular theory.\n\nNotable, and sometimes controversial, computer simulations used in science include: [[Donella Meadows]]' [[World3]] used in the ''[[Limits to Growth]]'', [[James Lovelock|James Lovelock's]] [[Daisyworld]] and Thomas Ray's [[Tierra (computer simulation)|Tierra]].\n\nIn social sciences, computer simulation is an integral component of the five angles of analysis fostered by the data percolation methodology,<ref>Mesly, Olivier \n(2015). ''Creating Models in Psychological Research.'' United States: Springer Psychology: 126 pages. {{ISBN|978-3-319-15752-8}}</ref> which also includes qualitative and quantitative methods, reviews of the literature (including scholarly), and interviews with experts, and which forms an extension of data triangulation.\n\n=== Simulation environments for physics and engineering ===\n[[Graphical environment]]s to design simulations have been developed. Special care was taken to handle events (situations in which the simulation equations are not valid and have to be changed). The open project [[Open Source Physics]] was started to develop reusable libraries for simulations in [[Java (programming language)|Java]], together with [[EJS|Easy Java Simulations]], a complete graphical environment that generates code based on these libraries.\n\n=== Simulation environments for linguistics ===\nTaiwanese Tone Group Parser<ref>Chang, Y. C. (2017). A Knowledge Representation Method to Implement A Taiwanese Tone Group Parser [In Chinese]. International Journal of Computational Linguistics & Chinese Language Processing; 22:2 2017.12 pp. 73\u201386</ref> is a simulator of Taiwanese tone sandhi acquisition.\nIn practical, the method using linguistic theory to implement the Taiwanese tone group parser is a way to apply [[knowledge engineering]] technique to build the experiment environment of computer simulation for language acquisition. A work-in-process version of artificial tone group parser that includes a [[knowledge base]] and an executable program file for Microsoft Windows system (XP/Win7) can be [http://vikon.myweb.hinet.net/ttgpe.htm download] for evaluation.\n\n== Computer simulation in practical contexts ==\nComputer simulations are used in a wide variety of practical contexts, such as:\n* analysis of [[air pollutant]] dispersion using [[atmospheric dispersion modeling]]\n* design of complex systems such as [[aircraft]] and also [[logistics]] systems.\n* design of [[noise barrier]]s to effect roadway [[noise mitigation]]\n* modeling of [[Application performance management|application performance]]<ref>{{cite book | last = Wescott | first = Bob | title = The Every Computer Performance Book, Chapter 7: Modeling Computer Performance | publisher = [[CreateSpace]] | date = 2013 | isbn = 978-1482657753 | url = https://books.google.com/books?id=0SD1mgEACAAJ}}</ref>\n* [[flight simulator]]s to train pilots\n* [[Atmospheric model|weather forecasting]]\n* [[risk management|forecasting of risk]]\n* simulation of electrical circuits\n* [[Power system simulation]]\n* simulation of other computers is [[Emulator|emulation]].\n* forecasting of prices on financial markets (for example [[Adaptive Modeler]])\n* behavior of structures (such as buildings and industrial parts) under stress and other conditions\n* design of industrial processes, such as chemical processing plants\n* [[strategic management]] and [[organizational studies]]\n* [[reservoir simulation]] for the petroleum engineering to model the subsurface reservoir\n* process engineering simulation tools.\n* [[Robotics suite|robot simulators]] for the design of robots and robot control algorithms\n* [[UrbanSim|urban simulation models]] that simulate dynamic patterns of urban development and responses to urban land use and transportation policies. See a more detailed article on [[Urban Environment Simulation]].\n* [[Traffic engineering (transportation)|traffic engineering]] to plan or redesign parts of the street network from single junctions over cities to a national highway network to transportation system planning, design and operations. See a more detailed article on [[Traffic Simulation|Simulation in Transportation]].\n* modeling car crashes to test safety mechanisms in new vehicle models.\n* [[Theoretical production ecology|crop-soil systems]] in agriculture, via dedicated software frameworks (e.g. [[Biophysical Models|BioMA]], OMS3, APSIM)\n\nThe reliability and the trust people put in computer simulations depends on the [[Validity (logic)|validity]] of the simulation [[model (abstract)|model]], therefore [[verification and validation]] are of crucial importance in the development of computer simulations. Another important aspect of computer simulations is that of reproducibility of the results, meaning that a simulation model should not provide a different answer for each execution. Although this might seem obvious, this is a special point of attention in [[stochastic simulation]]s, where random numbers should actually be semi-random numbers. An exception to reproducibility are human-in-the-loop simulations such as flight simulations and [[computer games]]. Here a human is part of the simulation and thus influences the outcome in a way that is hard, if not impossible, to reproduce exactly.\n\n[[Vehicle]] manufacturers make use of computer simulation to test safety features in new designs. By building a copy of the car in a physics simulation environment, they can save the hundreds of thousands of dollars that would otherwise be required to build and test a unique prototype. Engineers can step through the simulation milliseconds at a time to determine the exact stresses being put upon each section of the prototype.<ref>Baase, Sara. A Gift of Fire: Social, Legal, and Ethical Issues for Computing and the Internet. 3. Upper Saddle River: Prentice Hall, 2007. Pages 363\u2013364. {{ISBN|0-13-600848-8}}.</ref>\n\n[[Computer graphics]] can be used to display the results of a computer simulation. [[Animations]] can be used to experience a simulation in real-time, e.g., in [[Training Simulation|training simulations]]. In some cases animations may also be useful in faster than real-time or even slower than real-time modes. For example, faster than real-time animations can be useful in visualizing the buildup of queues in the simulation of humans evacuating a building. Furthermore, simulation results are often aggregated into static images using various ways of [[scientific visualization]].\n\nIn debugging, simulating a program execution under test (rather than executing natively) can detect far more errors than the hardware itself can detect and, at the same time, log useful debugging information such as instruction trace, memory alterations and instruction counts. This technique can also detect [[buffer overflow]] and similar \"hard to detect\" errors as well as produce performance information and [[Performance tuning|tuning]] data.\n\n== Pitfalls ==\nAlthough sometimes ignored in computer simulations, it is very important to perform a [[sensitivity analysis]] to ensure that the accuracy of the results is properly understood. For example, the probabilistic risk analysis of factors determining the success of an oilfield exploration program involves combining samples from a variety of statistical distributions using the [[Monte Carlo method]]. If, for instance, one of the key parameters (e.g., the net ratio of oil-bearing strata) is known to only one significant figure, then the result of the simulation might not be more precise than one significant figure, although it might (misleadingly) be presented as having four significant figures.\n\n=== Model calibration techniques ===\nThe following three steps should be used to produce accurate simulation models: calibration, verification, and validation. Computer simulations are good at portraying and comparing theoretical scenarios, but in order to accurately model actual case studies they have to match what is actually happening today. A base model should be created and calibrated so that it matches the area being studied. The calibrated model should then be verified to ensure that the model is operating as expected based on the inputs. Once the model has been verified, the final step is to validate the model by comparing the outputs to historical data from the study area. This can be done by using statistical techniques and ensuring an adequate R-squared value. Unless these techniques are employed, the simulation model created will produce inaccurate results and not be a useful prediction tool.\n\nModel calibration is achieved by adjusting any available parameters in order to adjust how the model operates and simulates the process. For example, in traffic simulation, typical parameters include look-ahead distance, car-following sensitivity, discharge headway, and start-up lost time. These parameters influence driver behavior such as when and how long it takes a driver to change lanes, how much distance a driver leaves between his car and the car in front of it, and how quickly a driver starts to accelerate through an intersection. Adjusting these parameters has a direct effect on the amount of traffic volume that can traverse through the modeled roadway network by making the drivers more or less aggressive. These are examples of calibration parameters that can be fine-tuned to match characteristics observed in the field at the study location. Most traffic models have typical default values but they may need to be adjusted to better match the driver behavior at the specific location being studied.\n\nModel verification is achieved by obtaining output data from the model and comparing them to what is expected from the input data. For example, in traffic simulation, traffic volume can be verified to ensure that actual volume throughput in the model is reasonably close to traffic volumes input into the model. Ten percent is a typical threshold used in traffic simulation to determine if output volumes are reasonably close to input volumes. Simulation models handle model inputs in different ways so traffic that enters the network, for example, may or may not reach its desired destination. Additionally, traffic that wants to enter the network may not be able to, if congestion exists. This is why model verification is a very important part of the modeling process.\n\nThe final step is to validate the model by comparing the results with what is expected based on historical data from the study area. Ideally, the model should produce similar results to what has happened historically. This is typically verified by nothing more than quoting the R-squared statistic from the fit. This statistic measures the fraction of variability that is accounted for by the model. A high R-squared value does not necessarily mean the model fits the data well. Another tool used to validate models is graphical residual analysis. If model output values drastically differ from historical values, it probably means there is an error in the model. Before using the model as a base to produce additional models, it is important to verify it for different scenarios to ensure that each one is accurate. If the outputs do not reasonably match historic values during the validation process, the model should be reviewed and updated to produce results more in line with expectations. It is an iterative process that helps to produce more realistic models.\n\nValidating traffic simulation models requires comparing traffic estimated by the model to observed traffic on the roadway and transit systems. Initial comparisons are for trip interchanges between quadrants, sectors, or other large areas of interest. The next step is to compare traffic estimated by the models to traffic counts, including transit ridership, crossing contrived barriers in the study area. These are typically called screenlines, cutlines, and cordon lines and may be imaginary or actual physical barriers. Cordon lines surround particular areas such as a city's central business district or other major activity centers. Transit ridership estimates are commonly validated by comparing them to actual patronage crossing cordon lines around the central business district.\n\nThree sources of error can cause weak correlation during calibration: input error, model error, and parameter error. In general, input error and parameter error can be adjusted easily by the user. Model error however is caused by the methodology used in the model and may not be as easy to fix. Simulation models are typically built using several different modeling theories that can produce conflicting results. Some models are more generalized while others are more detailed. If model error occurs as a result, in may be necessary to adjust the model methodology to make results more consistent.\n\nIn order to produce good models that can be used to produce realistic results, these are the necessary steps that need to be taken in order to ensure that simulation models are functioning properly. Simulation models can be used as a tool to verify engineering theories, but they are only valid if calibrated properly. Once satisfactory estimates of the parameters for all models have been obtained, the models must be checked to assure that they adequately perform the intended functions. The validation process establishes the credibility of the model by demonstrating its ability to replicate reality. The importance of model validation underscores the need for careful planning, thoroughness and accuracy of the input data collection program that has this purpose. Efforts should be made to ensure collected data is consistent with expected values. For example, in traffic analysis it is typical for a traffic engineer to perform a site visit to verify traffic counts and become familiar with traffic patterns in the area. The resulting models and forecasts will be no better than the data used for model estimation and validation.\n\n== See also ==\n\n[[File:Typhoon Mawar 2005 computer simulation thumbnail.gif|300px|thumb|A 48-hour computer simulation of [[2005 Pacific typhoon season#Typhoon Mawar|Typhoon Mawar]] using the [[Weather Research and Forecasting model]] ]]\n{{div col|colwidth=30em}}\n* [[Computational model]]\n* [[Emulator]]\n* [[Energy modeling]]\n* [[Illustris project]]\n* [[List of computer simulation software]]\n* [[Stencil code]]\n* [[UniverseMachine]]\n* [[Virtual prototyping]]\n* [[Web-based simulation]]\n{{div col end}}\n\n== References ==\n{{More footnotes|date=May 2008}}\n{{Reflist}}\n\n== Further reading ==\n{{Commons category}}\n* Kafashan, J.; Wi\u0105cek, J.; Abd Rahman, N.; Gan, J. (2019). \"[https://link.springer.com/article/10.1007/s10035-019-0935-1 Two-dimensional particle shapes modelling for DEM simulations in engineering: a review]\". Granular Matter. 21: 80. doi: 10.1007/s10035-019-0935-1\n*[https://www.taylorfrancis.com/books/9781351241120/ \"Modeling and Simulation\"], G. Dubois, Taylor & Francis, CRC Press, 2018.\n* [http://www.cuideas.org/publications/ \"A Resource Allocation Framework for Experiment-Based Validation of Numerical Models,\"] Journal of Mechanics of Advanced Materials and Structures (Taylor & Francis).\n* Young, Joseph and Findley, Michael. 2014. \"Computational Modeling to Study Conflicts and Terrorism.\" [https://books.google.com/books?id=ENDpAwAAQBAJ&pg=PT23 Routledge Handbook of Research Methods in Military Studies] edited by Soeters, Joseph; Shields, Patricia and Rietjens, Sebastiaan. pp.&nbsp;249\u2013260. New York: Routledge,\n* R. Frigg and S. Hartmann, [http://plato.stanford.edu/entries/models-science/ Models in Science]. Entry in the '' [[Stanford Encyclopedia of Philosophy]]''.\n*E. Winsberg [http://plato.stanford.edu/entries/simulations-science/ Simulation in Science]. Entry in the '' [[Stanford Encyclopedia of Philosophy]]''.\n* A.K. Hartmann, [https://web.archive.org/web/20090211113048/http://worldscibooks.com/physics/6988.html Practical Guide to Computer Simulations], Singapore: [[World Scientific]], 2009\n* S. Hartmann, [http://philsci-archive.pitt.edu/archive/00002412/ The World as a Process: Simulations in the Natural and Social Sciences], in: R. Hegselmann et al. (eds.), ''Modelling and Simulation in the Social Sciences from the Philosophy of Science Point of View'', Theory and Decision Library. Dordrecht: [[Kluwer]] 1996, 77\u2013100.\n* E. Winsberg, ''Science in the Age of Computer Simulation''. Chicago: [[University of Chicago Press]], 2010.\n* P. Humphreys, ''Extending Ourselves: Computational Science, Empiricism, and Scientific Method''. Oxford: [[Oxford University Press]], 2004.\n* {{cite book|author=James J. Nutaro|title=Building Software for Simulation: Theory and Algorithms, with Applications in C++|url=https://books.google.com/books?id=WZceCd74GRcC|year=2011|publisher=John Wiley & Sons|isbn=978-1-118-09945-2}}\n* Desa, W. L. H. M., Kamaruddin, S., & Nawawi, M. K. M. (2012). Modeling of Aircraft Composite Parts Using Simulation. Advanced Material Research, 591\u2013593, 557\u2013560.\n\n{{Computer simulation}}\n{{Energy modeling}}\n{{Authority control}}\n\n{{DEFAULTSORT:Computer Simulation}}\n[[Category:Computational science]]\n[[Category:Scientific modeling]]\n[[Category:Simulation software| ]]\n[[Category:Virtual reality]]\n[[Category:Alternatives to animal testing]]\n[[Category:Computational fields of study]]\n", "name_user": "Rjwilmsi", "label": "safe", "comment": "Journal cites:, templated 2 journal cites", "url_page": "//en.wikipedia.org/wiki/Computer_simulation"}
