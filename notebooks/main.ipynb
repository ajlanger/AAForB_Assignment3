{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main script to call functions from other scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize other notebooks\n",
    "# ------------ Initialize preprocessing script\n",
    "%run \"Preprocessing.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full pipeline\n",
    "Extracting edited part --> Lemmatization, stemming and removing stopwords --> TF-IDF --> Feed to model --> Return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals()['models_loaded'] = False\n",
    "\n",
    "# ------------ Replace this with a trained ML model\n",
    "def predict(df):\n",
    "    if any([x in df.diff.lower() for x in ['bad', 'lol', 'joke']]):\n",
    "        return 'vandal'\n",
    "    else:\n",
    "        return 'safe'\n",
    "# ------------ Replace this with a trained ML model\n",
    "\n",
    "predict_udf = udf(predict, StringType())\n",
    "\n",
    "def process(time, rdd):\n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "    This function will serve as the construction pipeline.\n",
    "    Here all preprocessing functions need to be called and \n",
    "    need to be able to handle one wiki edit.\n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "    if rdd.isEmpty():\n",
    "        return\n",
    "    \n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    \n",
    "    #########################################################\n",
    "    # PREPROCESSING & FEATURE ENGINEERING\n",
    "    #########################################################\n",
    "    df = spark.read.json(rdd)\n",
    "    \n",
    "    clean_df = get_clean_df(df)\n",
    "    df_with_difference = get_difference_column(clean_df)\n",
    "    final_df = split_difference_into_removed_added(df_with_difference)\n",
    "    final_df.show(3)\n",
    "    \n",
    "    #########################################################\n",
    "    # MAKE PREDICTION\n",
    "    #########################################################\n",
    "    \n",
    "#     # Utilize our predict function\n",
    "#     df_withpreds = df_withdiff.withColumn(\"pred\", predict_udf(\n",
    "#         struct([df_withdiff[x] for x in df_withdiff.columns])\n",
    "#     ))\n",
    "#     df_withpreds.show()\n",
    "    \n",
    "#     # Normally, you wouldn't use a UDF (User Defined Function) Python function to predict (you can)\n",
    "#     # But an MLlib model you've built and saved with Spark\n",
    "#     # In this case, you need to prevent loading your model in every call to \"process\" as follows:\n",
    "    \n",
    "#     # Load in the model if not yet loaded:\n",
    "#     if not globals()['models_loaded']:\n",
    "#         # load in your models here\n",
    "#         globals()['my_model'] = '***' # Replace '***' with:    [...].load('my_logistic_regression')\n",
    "#         globals()['models_loaded'] = True\n",
    "        \n",
    "#     # And then predict using the loaded model: \n",
    "#     # df_result = globals()['my_model'].transform(df)\n",
    "#     # df_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin online stream and make prediction per wiki edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2020-04-30 09:59:00 =========\n",
      "+-----+------------+--------------------+-----------+--------------------+--------------------+-------------+-----------+\n",
      "|label|     comment|          title_page|  name_user|      clean_old_text|      clean_new_text|removed_words|added_words|\n",
      "+-----+------------+--------------------+-----------+--------------------+--------------------+-------------+-----------+\n",
      "| safe|active cases|2020 coronavirus ...|maranello10|pp-protectedsmall...|pp-protectedsmall...|  nd ha n't  |       or h|\n",
      "+-----+------------+--------------------+-----------+--------------------+--------------------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start the prediction task\n",
    "ssc = StreamingContext(sc, 10) # ---------------------> Get minibatches every 10 seconds\n",
    "lines = ssc.socketTextStream(\"seppe.net\", 7778)\n",
    "lines.foreachRDD(process) # --------------------------> Perform the function 'process' on every RDD\n",
    "ssc_t = StreamingThread(ssc)\n",
    "ssc_t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Stopping... this may take a few seconds -----\n",
      "========= 2020-04-30 09:59:10 =========\n",
      "+-----+---------------+--------------------+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "|label|        comment|          title_page|name_user|      clean_old_text|      clean_new_text|       removed_words|         added_words|\n",
      "+-----+---------------+--------------------+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "| safe|election result|abdullah ahmad ba...|     nctb|short description...|short description...|rowspan3'''p35 ke...|rowspan9'''kepala...|\n",
      "| safe|          empty|    guangxi massacre| screditc|the '''guangxi ma...|the '''guangxi ma...| during the bolua...|               empty|\n",
      "+-----+---------------+--------------------+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stop the prediction task\n",
    "ssc_t.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
