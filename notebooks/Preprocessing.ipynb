{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has following goals:\n",
    "- Get difference between old and new wiki page\n",
    "- Cleaning (lower cases, removing stopwords)\n",
    "- Lemmatizing\n",
    "- TF-IDF\n",
    "- Feature engineering\n",
    "- ... from: The acquired training data AND future incoming streaming instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import libraries\n",
    "from pyspark.streaming import StreamingContext\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "from threading import Thread\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, struct, array, col, lit, regexp_replace, lower\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from difflib import unified_diff\n",
    "import difflib\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from pyspark.sql import types\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from pyspark.sql.functions import monotonically_increasing_id \n",
    "import itertools\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Get streaming instances\n",
    "This section is only for the purpose to set up and do intermediate checks of the pipeline with live data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This only serves as trial to test my functions on live incoming instances\n",
    "class StreamingThread(Thread):\n",
    "    def __init__(self, ssc):\n",
    "        Thread.__init__(self)\n",
    "        self.ssc = ssc\n",
    "    # Start stream\n",
    "    def run(self):\n",
    "        ssc.start()\n",
    "        ssc.awaitTermination()\n",
    "    # Stop stream\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)\n",
    "\n",
    "# ssc = StreamingContext(sc, 10) # Every 10 seconds, construct a mini-batch of RDDs\n",
    "# lines = ssc.socketTextStream(\"seppe.net\", 7778)\n",
    "# lines.pprint()\n",
    "# ssc_t = StreamingThread(ssc)\n",
    "# ssc_t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ssc_t.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load in the collected data\n",
    "This part only serves to load in our collected data: to then experiment and perform featurization, TF-IDF and construct classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Spark dataframe --> various sites state that a dataframe is better than a DDR for textual data\n",
    "def get_wiki_df():\n",
    "    # The path can be either a single text file or a directory storing text files. \n",
    "    # It's possible you'll need to change path to your own directory\n",
    "    path = \"../../data/*\"\n",
    "    wiki_df = spark.read.json(path)\n",
    "\n",
    "    # Uncomment if you want its schema\n",
    "    # wiki_df.printSchema()\n",
    "    return wiki_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Check the amount of label counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_count(wiki_df):\n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "    Last result (so you don't need to run it each time) \n",
    "    -> safe: 30333, unsafe: 4136, vandal: 270\n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "    # Creates a temporary view using the wiki DataFrame\n",
    "    wiki_df.createOrReplaceTempView(\"wikidata\")                            \n",
    "                                                                                  \n",
    "    # Check the amount of safes/unsafes/vandals                                    \n",
    "    label_df = spark.sql(\"SELECT label, count(*) FROM wikidata GROUP BY label\")    \n",
    "    return label_df.show()                                                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helperfunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "# HELPER FUNCTIONS\n",
    "######################################################################################\n",
    "\n",
    "def show_X_rows(df, x): #--------------------------------> Shows first X number of spark dataframe rows\n",
    "    # Convert list to RDD\n",
    "    rdd = spark.sparkContext.parallelize(df.take(x))\n",
    "\n",
    "    # Create data frame\n",
    "    df_temp = spark.createDataFrame(rdd)\n",
    "    return df_temp.show()\n",
    "\n",
    "def flatten(nested_list):\n",
    "    flat_list = []\n",
    "    for sublist in nested_list:\n",
    "        if type(sublist) == list:\n",
    "            for item in sublist:\n",
    "                flat_list.append(item)\n",
    "        else:\n",
    "            flat_list.append(sublist)\n",
    "    return flat_list\n",
    "\n",
    "######################################################################################\n",
    "# FUNCTIONS TO GET DIFFERENCE BETWEEN OLD AND NEW TEXT\n",
    "######################################################################################\n",
    "\n",
    "# Get edited part of wiki page (credits to the professor)\n",
    "def get_diff_1(old, new):\n",
    "    return '\\n'.join([ l for l in unified_diff(old.split('\\n'), new.split('\\n')) if l.startswith('+') or l.startswith('-') ])\n",
    "\n",
    "# The difference function that will take a very long time to compute for all instances\n",
    "def get_diff_2(old, new):\n",
    "    deleted_words = []\n",
    "    added_words = []\n",
    "    temp_i = -100\n",
    "    new_word = ''\n",
    "    status = 'none'\n",
    "    for i, s in enumerate(difflib.ndiff(old, new)):\n",
    "        if s[0] == ' ':\n",
    "            if status == 'adding':\n",
    "                added_words.append(new_word)\n",
    "                new_word = ''\n",
    "            elif status == 'deleting':\n",
    "                deleted_words.append(new_word)\n",
    "                new_word = ''\n",
    "            status = 'none'\n",
    "            continue\n",
    "        elif s[0] == '-':\n",
    "            if status != 'deleting':\n",
    "                added_words.append(new_word)\n",
    "                new_word = ''\n",
    "                status = 'deleting'\n",
    "            new_word += s[-1]\n",
    "        elif s[0] == '+':\n",
    "            if status != 'adding':\n",
    "                deleted_words.append(new_word)\n",
    "                new_word = ''\n",
    "                status = 'adding'\n",
    "            new_word += s[-1]\n",
    "    if new_word != '' and status == 'deleting':\n",
    "        deleted_words.append(new_word)\n",
    "    elif new_word != '' and status == 'adding':\n",
    "        added_words.append(new_word)\n",
    "    return {'added': added_words, 'deleted': deleted_words}\n",
    "\n",
    "def get_deletions(del_add):\n",
    "    return del_add['deleted']\n",
    "\n",
    "def get_additions(del_add):\n",
    "    return del_add['added']\n",
    "\n",
    "def extract_differences(str_old, str_new):\n",
    "    try:\n",
    "        # Clean old and new wiki pages from html characters\n",
    "        str_old, str_new = cleanhtml(str_old), cleanhtml(str_new)\n",
    "\n",
    "        # Get big chunks of altered fragments\n",
    "        diff1 = get_diff_1(str_old, str_new)\n",
    "\n",
    "\n",
    "        # Clean the differences from unwanted characters to get only words\n",
    "        diff2 = []\n",
    "        for txt in diff1.split('\\n'):\n",
    "            diff2.append(''.join([x for x in txt if x in string.ascii_letters + '\\'-+ 1234567890']).lower())\n",
    "\n",
    "        # Find chunks that are related ('+' vs '-') else append them to a seperate list that doesn't need to get processed in the next part\n",
    "        fully_added, fully_removed, partly_new, partly_old = find_similar_chunks(diff2)\n",
    "\n",
    "        # Get the individual fragments that were added or deleted\n",
    "        for i in range(0,len(partly_new)):\n",
    "            difference = get_diff_2(partly_new[i], partly_old[i])\n",
    "            for el in difference['added']:\n",
    "                fully_added.append(el)\n",
    "            for el in difference['deleted']:\n",
    "                fully_removed.append(el)\n",
    "\n",
    "        fully_removed = flatten(fully_removed)\n",
    "        fully_added = flatten(fully_added)\n",
    "        while '' in fully_added:\n",
    "            fully_added.remove('')\n",
    "        while '+' in fully_added:\n",
    "            fully_added.remove('+')\n",
    "        while '-' in fully_added:\n",
    "            fully_added.remove('-')\n",
    "\n",
    "        while '' in fully_removed:\n",
    "            fully_removed.remove('')\n",
    "        while '+' in fully_removed:\n",
    "            fully_removed.remove('+')\n",
    "        while '-' in fully_removed:\n",
    "            fully_removed.remove('-')\n",
    "\n",
    "        # Original output_lib was a dictionary\n",
    "        #output_lib = {'added': fully_added, 'removed': fully_removed}\n",
    "\n",
    "        output_str = ''\n",
    "        # However, now we'd like to have the output in a list with seperator \"|SEPERATIONLINEADDEDREMOVED|\"\n",
    "        for word in fully_added:\n",
    "            output_str += '{} '.format(word)\n",
    "\n",
    "        output_str += ' |SEPERATIONLINEADDEDREMOVED|'\n",
    "\n",
    "        for word in fully_removed:\n",
    "            output_str += ' {}'.format(word)\n",
    "\n",
    "        full_difference = []\n",
    "\n",
    "        return output_str\n",
    "    except:\n",
    "        return 'error_would_occur'\n",
    "\n",
    "#     for i in output_lib['added']:\n",
    "#         full_difference.append(i)\n",
    "#     for i in output_lib['removed']:\n",
    "#         full_difference.append(i)\n",
    "\n",
    "#     return full_difference\n",
    "\n",
    "def find_similar_chunks(chunk_list):\n",
    "    fully_added, fully_removed, partly_removed, partly_added = [], [], [], []\n",
    "    # Remove '', ---, +++\n",
    "    while '' in chunk_list:\n",
    "        chunk_list.remove('')\n",
    "    while '+++ ' in chunk_list:\n",
    "        chunk_list.remove('+++ ')\n",
    "    while '--- ' in chunk_list:\n",
    "        chunk_list.remove('--- ')\n",
    "    for chunk in chunk_list:\n",
    "        try:\n",
    "            if chunk[3] not in string.ascii_letters + '\\'-+ 1234567890':\n",
    "                chunk_list.remove(chunk)\n",
    "            else:\n",
    "                continue\n",
    "        except:\n",
    "            chunk_list.remove(chunk)\n",
    "\n",
    "    for a, b in itertools.combinations(chunk_list, 2):\n",
    "        if a[1:30] == b[1:30]:\n",
    "            if a[0] == '+':\n",
    "                partly_added.append(a)\n",
    "                partly_removed.append(b)\n",
    "                chunk_list.remove(a)\n",
    "                chunk_list.remove(b)\n",
    "            else:\n",
    "                partly_added.append(b)\n",
    "                partly_removed.append(a)\n",
    "                chunk_list.remove(a)\n",
    "                chunk_list.remove(b)\n",
    "    for rest in chunk_list:\n",
    "        if rest[0] == '-':\n",
    "            fully_removed.append(rest[1:].split(' '))\n",
    "        elif rest[0] == '+':\n",
    "            fully_added.append(rest[1:].split(' '))\n",
    "    return fully_added, fully_removed, partly_removed, partly_added # The partly removed and partly added ones will be used as input to determine difference\n",
    "\n",
    "######################################################################################\n",
    "# FUNCTIONS TO CLEAN COLUMNS 'comment', 'title', 'user', 'text_old', 'text_new'\n",
    "######################################################################################\n",
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    \n",
    "    # Remove urls\n",
    "    cleantext_no_urls = re.sub(r\"http\\S+\", \"\", cleantext)\n",
    "    return cleantext_no_urls\n",
    "\n",
    "def cleantext(raw):\n",
    "    # If there's nothing in raw, return 'EMPTY'\n",
    "    for item in 'azertyuiopmlkjhgfdsqnbvcxw,;:=ùµ$^)àç!è§(é&1234567890\\\"\\')|@#]}{[^-_':\n",
    "        if item in raw:\n",
    "    \n",
    "            cleanr = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "            cleantext = re.sub(cleanr, '', raw)\n",
    "\n",
    "            # Remove urls\n",
    "            cleantext_no_urls = re.sub(r\"http\\S+\", \"\", cleantext)\n",
    "            return ''.join([x for x in cleantext_no_urls if x in string.ascii_letters + '\\'-+ 1234567890']).lower()\n",
    "    return 'empty'\n",
    "\n",
    "def get_clean_df(df):\n",
    "    # Remove url_page column\n",
    "    df_without_url = df.drop('url_page')\n",
    "\n",
    "    # Cleaning comment, title_page and name_user\n",
    "    clean_udf = udf(cleantext, StringType())\n",
    "    df_without_url = df_without_url.withColumn('clean_comment', clean_udf(df_without_url.comment)).drop('comment')\n",
    "    df_without_url = df_without_url.withColumn('clean_title_page', clean_udf(df_without_url.title_page)).drop('title_page')\n",
    "    df_without_url = df_without_url.withColumn('clean_name_user', clean_udf(df_without_url.name_user)).drop('name_user')\n",
    "    \n",
    "    # Clean the old and new text columns\n",
    "    df_without_url = df_without_url.withColumn('clean_old_text', clean_udf(df_without_url.text_old))\n",
    "    df_without_url = df_without_url.withColumn('clean_new_text', clean_udf(df_without_url.text_new))\n",
    "    \n",
    "    data = df_without_url.select(col(\"label\"), col(\"clean_comment\").alias(\"comment\"), col(\"clean_title_page\").alias(\"title_page\"), col(\"clean_name_user\").alias(\"name_user\"), col(\"text_old\"), col(\"text_new\"), col(\"clean_old_text\"), col(\"clean_new_text\"))\n",
    "    return data\n",
    "\n",
    "def get_difference_column(df):\n",
    "    difference_udf = udf(extract_differences, StringType())\n",
    "    intermediate_col = df.withColumn('difference', difference_udf(df.text_old, df.text_new))\n",
    "    intermediate_col = intermediate_col.drop('text_old')\n",
    "    intermediate_col = intermediate_col.drop('text_new')\n",
    "    return intermediate_col\n",
    "\n",
    "def paste_words(list_of_words):\n",
    "    return ' '.join([x for x in list_of_words])\n",
    "\n",
    "######################################################################################\n",
    "# FUNCTIONS TO SPIT DIFFERENCE COLUMN INTO ADDED AND REMOVED COLUMN\n",
    "######################################################################################\n",
    "\n",
    "def get_removed_col(col):\n",
    "    for item in 'azertyuiopmlkjhgfdsqnbvcxw,;:=ùµ$^)àç!è§(é&1234567890\\\"\\')|@#]}{[^-_':\n",
    "            if item in col[:col.find('|SEPERATIONLINEADDEDREMOVED|')]:\n",
    "                return col[:col.find('|SEPERATIONLINEADDEDREMOVED|')].split(' ')\n",
    "    return ['empty']\n",
    "\n",
    "def get_added_col(col):\n",
    "    for item in 'azertyuiopmlkjhgfdsqnbvcxw,;:=ùµ$^)àç!è§(é&1234567890\\\"\\')|@#]}{[^-_':\n",
    "            if item in col[col.find('|SEPERATIONLINEADDEDREMOVED|') + len('|SEPERATIONLINEADDEDREMOVED|') + 1:]:\n",
    "                return col[col.find('|SEPERATIONLINEADDEDREMOVED|') + len('|SEPERATIONLINEADDEDREMOVED|') + 1:].split(' ')\n",
    "    return ['empty']\n",
    "\n",
    "def split_difference_into_removed_added(df):\n",
    "    get_removed_udf = udf(get_removed_col, types.ArrayType(types.StringType()).simpleString())\n",
    "    df = df.withColumn('removed_words', get_removed_udf(df.difference))\n",
    "    \n",
    "    get_added_udf = udf(get_added_col, types.ArrayType(types.StringType()).simpleString())\n",
    "    df = df.withColumn('added_words', get_added_udf(df.difference))\n",
    "    \n",
    "    df = df.drop('difference')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data as spark dataframe\n",
    "#wiki_df = get_wiki_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get clean dataframe (cleaning of comment, title_page, name_user):\n",
    "clean_df = get_clean_df(wiki_df)\n",
    "\n",
    "# In order to get the actual difference column\n",
    "df_with_difference = get_difference_column(clean_df)\n",
    "\n",
    "# Example of a difference column of the first of 20 instances:\n",
    "# difference column is in the form REMOVED PART |SEPERATIONLINEADDEDREMOVED| ADDED PART\n",
    "# df_with_difference.show(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|label|             comment|          title_page|         name_user|      clean_old_text|      clean_new_text|       removed_words|         added_words|\n",
      "+-----+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| safe|   fixed cite errors|timeline of the 2...|         john b123|short description...|short description...|[last, websitewww...|[cnbeta, lastcnn,...|\n",
      "| safe|duplicate word re...|timeline of the 2...|           arjayay|short description...|short description...|             [empty]|             [the, ]|\n",
      "| safe|         23 february|timeline of the 2...|    reddyhakky1998|see alsotimeline ...|see alsotimeline ...|    [february, , , ]|        [february, ]|\n",
      "| safe|4 februaryadded w...|timeline of the 2...|sebastianrueckoldt|see alsotimeline ...|see alsotimeline ...|[world, health, o...|             [empty]|\n",
      "+-----+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 columns: removed and added with space in new function. + Clean text_old and text_new\n",
    "# Split difference column into column 'removed' and column 'added'\n",
    "final_df = split_difference_into_removed_added(df_with_difference)\n",
    "\n",
    "final_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('label', 'string'),\n",
       " ('comment', 'string'),\n",
       " ('title_page', 'string'),\n",
       " ('name_user', 'string'),\n",
       " ('clean_old_text', 'string'),\n",
       " ('clean_new_text', 'string'),\n",
       " ('removed_words', 'array<string>'),\n",
       " ('added_words', 'array<string>')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final_df.filter(df_removed_added_colls.label == 'vandal').show(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
