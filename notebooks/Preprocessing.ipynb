{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook has following goals:\n",
    "\n",
    "- Get difference between old and new wiki page\n",
    "- Cleaning (lower cases, removing stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data manipulation\n",
    "\n",
    "from difflib import unified_diff\n",
    "import difflib\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import itertools\n",
    "\n",
    "# visualize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# pyspark\n",
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, struct, array, col, lit, regexp_replace, lower\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import monotonically_increasing_id \n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# other\n",
    "from threading import Thread\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj8AAABICAYAAAAZFJRnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAACa0lEQVR4nO3coYpUcRjG4W92TQabhi3CWrwADSYVtnkDGjQJVoPCYBoE4X8DimB32SyYTjHIBrvYbIIYdEEwyfEGxGHDf45z3ueJwwy8X/sxZ5jFOI5jAQCE2Jl6AADAJokfACCK+AEAoogfACCK+AEAoogfACDKmXVvGIahhmGoqqrWWvdBAAA9LU77Pz87j9702jK5X4/36svhtalndLF357jOvj2cekY3P2/crs+rq1PP6Obis5P69PzD1DO6ufzwUn08ejr1jG6u3H1Zv999m3pGF7vXz9eL16+mntHNg/s36+TrvalndPPj3HE9ef996hndHN268NfXPfYCAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgymIcx/FfbxiGoYZhqKqq1tpGRgEA9LL2m5+Dg4NqrVVrrZbL5SY2TWbO9835tir3bTv3ba8531blvrny2AsAiCJ+AIAou6vVanWaD+zv73ea8n+Y831zvq3KfdvOfdtrzrdVuW+O1v7gGQBgTjz2AgCiiB8AIIr4AQCiiB8AIIr4AQCi/AFDZVOIbKU9nwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# global variables (use capital letter for variables)\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "current_palette = sns.color_palette(\"colorblind\")\n",
    "sns.palplot(current_palette)\n",
    "sns.set_palette(\"colorblind\")\n",
    "DPI = 500 # determines quality when saving figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Get streaming instances\n",
    "This section is only for the purpose to set up and do intermediate checks of the pipeline with live data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This only serves as trial to test my functions on live incoming instances\n",
    "class StreamingThread(Thread):\n",
    "    def __init__(self, ssc):\n",
    "        Thread.__init__(self)\n",
    "        self.ssc = ssc\n",
    "    # Start stream\n",
    "    def run(self):\n",
    "        ssc.start()\n",
    "        ssc.awaitTermination()\n",
    "    # Stop stream\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)\n",
    "\n",
    "# ssc = StreamingContext(sc, 10) # Every 10 seconds, construct a mini-batch of RDDs\n",
    "# lines = ssc.socketTextStream(\"seppe.net\", 7778)\n",
    "# lines.pprint()\n",
    "# ssc_t = StreamingThread(ssc)\n",
    "# ssc_t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ssc_t.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Spark dataframe --> various sites state that a dataframe is better than a DDR for textual data\n",
    "\n",
    "def get_wiki_df(path = \"../../data/*\"):\n",
    "    \n",
    "    \"\"\" reads in the data\n",
    "    Args\n",
    "        path (str): the path can be either a single text file or a directory storing text files. \n",
    "    Returns\n",
    "        (pyspark.sql.dataframe.DataFrame) textual data\n",
    "    \"\"\"\n",
    "\n",
    "    wiki_df = spark.read.json(path)\n",
    "\n",
    "    # Uncomment if you want its schema\n",
    "    # wiki_df.printSchema()\n",
    "    return wiki_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_count(wiki_df):\n",
    "    \n",
    "    \"\"\"\n",
    "    For this function you should only pass the full \n",
    "    wiki_df in order to get the label count.\n",
    "    --------------------------------------\n",
    "    Last result (so you don't need to run it each time) \n",
    "    -> safe: 30333, unsafe: 4136, vandal: 270\n",
    "    \"\"\"\n",
    "    # Creates a temporary view using the wiki DataFrame\n",
    "    wiki_df.createOrReplaceTempView(\"wikidata\")                            \n",
    "                                                                                  \n",
    "    # Check the amount of safes/unsafes/vandals                                    \n",
    "    label_df = spark.sql(\"SELECT label, count(*) FROM wikidata GROUP BY label\")    \n",
    "    return label_df                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_X_rows(df, x): \n",
    "    \n",
    "    \"\"\" \n",
    "    Shows first X number of spark dataframe rows\n",
    "    Input the dataframe followed by a \n",
    "    number in order to see the first x amount\n",
    "    of rows\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert list to RDD\n",
    "    rdd = spark.sparkContext.parallelize(df.take(x))\n",
    "\n",
    "    # Create data frame\n",
    "    df_temp = spark.createDataFrame(rdd)\n",
    "    return df_temp.show()\n",
    "\n",
    "def flatten(nested_list):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function that flattens a nested list.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    flat_list = []\n",
    "    for sublist in nested_list:\n",
    "        if type(sublist) == list:\n",
    "            for item in sublist:\n",
    "                flat_list.append(item)\n",
    "        else:\n",
    "            flat_list.append(sublist)\n",
    "            \n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Function to get the difference between old and new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get edited part of wiki page (credits to the professor)\n",
    "def get_diff_1(old, new):\n",
    "    \n",
    "    \"\"\"\"\n",
    "    Takes in old and new columns and \n",
    "    returns the difference between the two\n",
    "    \"\"\"\n",
    "    \n",
    "    return '\\n'.join([ l for l in unified_diff(old.split('\\n'), new.split('\\n')) if l.startswith('+') or l.startswith('-') ])\n",
    "\n",
    "# The difference function that will take a very long time to compute for all instances\n",
    "def get_diff_2(old, new):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes in old and new columns and \n",
    "    returns the difference between the two\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    deleted_words = []\n",
    "    added_words = []\n",
    "    temp_i = -100\n",
    "    new_word = ''\n",
    "    status = 'none'\n",
    "    for i, s in enumerate(difflib.ndiff(old, new)):\n",
    "        if s[0] == ' ':\n",
    "            if status == 'adding':\n",
    "                added_words.append(new_word)\n",
    "                new_word = ''\n",
    "            elif status == 'deleting':\n",
    "                deleted_words.append(new_word)\n",
    "                new_word = ''\n",
    "            status = 'none'\n",
    "            continue\n",
    "        elif s[0] == '-':\n",
    "            if status != 'deleting':\n",
    "                added_words.append(new_word)\n",
    "                new_word = ''\n",
    "                status = 'deleting'\n",
    "            new_word += s[-1]\n",
    "        elif s[0] == '+':\n",
    "            if status != 'adding':\n",
    "                deleted_words.append(new_word)\n",
    "                new_word = ''\n",
    "                status = 'adding'\n",
    "            new_word += s[-1]\n",
    "    if new_word != '' and status == 'deleting':\n",
    "        deleted_words.append(new_word)\n",
    "    elif new_word != '' and status == 'adding':\n",
    "        added_words.append(new_word)\n",
    "    return {'added': added_words, 'deleted': deleted_words}\n",
    "\n",
    "def get_deletions(del_add):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function that returns the deleted\n",
    "    words out of a list of deleted and added\n",
    "    words\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    return del_add['deleted']\n",
    "\n",
    "def get_additions(del_add):\n",
    "    \n",
    "    \"\"\"\"\n",
    "    Function that returns the added\n",
    "    words out of a list of deleted and added\n",
    "    words\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    return del_add['added']\n",
    "\n",
    "def extract_differences(str_old, str_new):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function that splits old and new string\n",
    "    and extracts its differences\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Clean old and new wiki pages from html characters\n",
    "        str_old, str_new = cleanhtml(str_old), cleanhtml(str_new)\n",
    "\n",
    "        # Get big chunks of altered fragments\n",
    "        diff1 = get_diff_1(str_old, str_new)\n",
    "\n",
    "\n",
    "        # Clean the differences from unwanted characters to get only words\n",
    "        diff2 = []\n",
    "        for txt in diff1.split('\\n'):\n",
    "            diff2.append(''.join([x for x in txt if x in string.ascii_letters + '\\'-+ 1234567890']).lower())\n",
    "\n",
    "        # Find chunks that are related ('+' vs '-') else append them to a seperate list that doesn't need to get processed in the next part\n",
    "        fully_added, fully_removed, partly_new, partly_old = find_similar_chunks(diff2)\n",
    "\n",
    "        # Get the individual fragments that were added or deleted\n",
    "        for i in range(0,len(partly_new)):\n",
    "            difference = get_diff_2(partly_new[i], partly_old[i])\n",
    "            for el in difference['added']:\n",
    "                fully_added.append(el)\n",
    "            for el in difference['deleted']:\n",
    "                fully_removed.append(el)\n",
    "\n",
    "        fully_removed = flatten(fully_removed)\n",
    "        fully_added = flatten(fully_added)\n",
    "        while '' in fully_added:\n",
    "            fully_added.remove('')\n",
    "        while '+' in fully_added:\n",
    "            fully_added.remove('+')\n",
    "        while '-' in fully_added:\n",
    "            fully_added.remove('-')\n",
    "\n",
    "        while '' in fully_removed:\n",
    "            fully_removed.remove('')\n",
    "        while '+' in fully_removed:\n",
    "            fully_removed.remove('+')\n",
    "        while '-' in fully_removed:\n",
    "            fully_removed.remove('-')\n",
    "\n",
    "        # Original output_lib was a dictionary\n",
    "        #output_lib = {'added': fully_added, 'removed': fully_removed}\n",
    "\n",
    "        output_str = ''\n",
    "        # However, now we'd like to have the output in a list with seperator \"|SEPERATIONLINEADDEDREMOVED|\"\n",
    "        for word in fully_added:\n",
    "            output_str += '{} '.format(word)\n",
    "\n",
    "        output_str += ' |SEPERATIONLINEADDEDREMOVED|'\n",
    "\n",
    "        for word in fully_removed:\n",
    "            output_str += ' {}'.format(word)\n",
    "\n",
    "        full_difference = []\n",
    "\n",
    "        return output_str\n",
    "    except:\n",
    "        return 'error_would_occur'\n",
    "\n",
    "def find_similar_chunks(chunk_list):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function that takes in the differences between two blocks of\n",
    "    text and returns the exact deletion and added chunks\n",
    "    Not important to understand this function, it was a means \n",
    "    to process the output from get_diff_1 in order to process \n",
    "    further, it speeds up the process\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    fully_added, fully_removed, partly_removed, partly_added = [], [], [], []\n",
    "    # Remove '', ---, +++\n",
    "    while '' in chunk_list:\n",
    "        chunk_list.remove('')\n",
    "    while '+++ ' in chunk_list:\n",
    "        chunk_list.remove('+++ ')\n",
    "    while '--- ' in chunk_list:\n",
    "        chunk_list.remove('--- ')\n",
    "    for chunk in chunk_list:\n",
    "        try:\n",
    "            if chunk[3] not in string.ascii_letters + '\\'-+ 1234567890':\n",
    "                chunk_list.remove(chunk)\n",
    "            else:\n",
    "                continue\n",
    "        except:\n",
    "            chunk_list.remove(chunk)\n",
    "\n",
    "    for a, b in itertools.combinations(chunk_list, 2):\n",
    "        if a[1:30] == b[1:30]:\n",
    "            if a[0] == '+':\n",
    "                partly_added.append(a)\n",
    "                partly_removed.append(b)\n",
    "                chunk_list.remove(a)\n",
    "                chunk_list.remove(b)\n",
    "            else:\n",
    "                partly_added.append(b)\n",
    "                partly_removed.append(a)\n",
    "                chunk_list.remove(a)\n",
    "                chunk_list.remove(b)\n",
    "    for rest in chunk_list:\n",
    "        if rest[0] == '-':\n",
    "            fully_removed.append(rest[1:].split(' '))\n",
    "        elif rest[0] == '+':\n",
    "            fully_added.append(rest[1:].split(' '))\n",
    "    return fully_added, fully_removed, partly_removed, partly_added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Clean columns: *comments, title, user, text_old, text_new*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cleanhtml(raw_html):\n",
    "    \n",
    "    \"\"\"\n",
    "    Clean a raw text part from html symbols and words\n",
    "    \"\"\"\n",
    "    \n",
    "    cleanr = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    \n",
    "    # Remove urls\n",
    "    cleantext_no_urls = re.sub(r\"http\\S+\", \"\", cleantext)\n",
    "    return cleantext_no_urls\n",
    "\n",
    "def cleantext(raw):\n",
    "    \n",
    "    \"\"\"\n",
    "    Clean a raw text part further from other unimportant characters\n",
    "    and make it lower case\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # If there's nothing in raw, return 'EMPTY'\n",
    "    for item in 'azertyuiopmlkjhgfdsqnbvcxw,;:=ùµ$^)àç!è§(é&1234567890\\\"\\')|@#]}{[^-_':\n",
    "        if item in raw:\n",
    "    \n",
    "            cleanr = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "            cleantext = re.sub(cleanr, '', raw)\n",
    "\n",
    "            # Remove urls\n",
    "            cleantext_no_urls = re.sub(r\"http\\S+\", \"\", cleantext)\n",
    "            return ''.join([x for x in cleantext_no_urls if x in string.ascii_letters + '\\'-+ 1234567890']).lower()\n",
    "    return 'empty'\n",
    "\n",
    "def get_clean_df(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function used to pass the dataframe and clean its\n",
    "    columns \n",
    "    \"\"\"\n",
    "\n",
    "    # Remove url_page column\n",
    "    df_without_url = df.drop('url_page')\n",
    "\n",
    "    # Cleaning comment, title_page and name_user\n",
    "    clean_udf = udf(cleantext, StringType())\n",
    "    df_without_url = df_without_url.withColumn('clean_comment', clean_udf(df_without_url.comment)).drop('comment')\n",
    "    df_without_url = df_without_url.withColumn('clean_title_page', clean_udf(df_without_url.title_page)).drop('title_page')\n",
    "    df_without_url = df_without_url.withColumn('clean_name_user', clean_udf(df_without_url.name_user)).drop('name_user')\n",
    "    \n",
    "    # Clean the old and new text columns\n",
    "    df_without_url = df_without_url.withColumn('clean_old_text', clean_udf(df_without_url.text_old))\n",
    "    df_without_url = df_without_url.withColumn('clean_new_text', clean_udf(df_without_url.text_new))\n",
    "    \n",
    "    data = df_without_url.select(col(\"label\"), col(\"clean_comment\").alias(\"comment\"), col(\"clean_title_page\").alias(\"title_page\"), col(\"clean_name_user\").alias(\"name_user\"), col(\"text_old\"), col(\"text_new\"), col(\"clean_old_text\"), col(\"clean_new_text\"))\n",
    "    return data\n",
    "\n",
    "def get_difference_column(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function that takes in the dataframe and returns\n",
    "    a dataframe with an extra 'difference' column\n",
    "    Drops the text_old and text_new columns\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    difference_udf = udf(extract_differences, StringType())\n",
    "    intermediate_col = df.withColumn('difference', difference_udf(df.text_old, df.text_new))\n",
    "    intermediate_col = intermediate_col.drop('text_old')\n",
    "    intermediate_col = intermediate_col.drop('text_new')\n",
    "    return intermediate_col\n",
    "\n",
    "def paste_words(list_of_words):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function that pastes together a list of words\n",
    "    \"\"\"\n",
    "  \n",
    "    return ' '.join([x for x in list_of_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Split *difference column* into added and removed column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_removed_col(col):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function that splits the 'difference' column and\n",
    "    returns the removed words\n",
    "    \"\"\"\n",
    "    \n",
    "    for item in 'azertyuiopmlkjhgfdsqnbvcxw,;:=ùµ$^)àç!è§(é&1234567890\\\"\\')|@#]}{[^-_':\n",
    "            if item in col[:col.find('|SEPERATIONLINEADDEDREMOVED|')]:\n",
    "                return col[:col.find('|SEPERATIONLINEADDEDREMOVED|')].split(' ')\n",
    "    return ['empty']\n",
    "\n",
    "def get_added_col(col):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function that splits the 'difference' column and\n",
    "    returns the added words\n",
    "    \"\"\"\n",
    "\n",
    "    for item in 'azertyuiopmlkjhgfdsqnbvcxw,;:=ùµ$^)àç!è§(é&1234567890\\\"\\')|@#]}{[^-_':\n",
    "            if item in col[col.find('|SEPERATIONLINEADDEDREMOVED|') + len('|SEPERATIONLINEADDEDREMOVED|') + 1:]:\n",
    "                return col[col.find('|SEPERATIONLINEADDEDREMOVED|') + len('|SEPERATIONLINEADDEDREMOVED|') + 1:].split(' ')\n",
    "    return ['empty']\n",
    "\n",
    "\n",
    "def split_difference_into_removed_added(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function that splits the difference column and adds\n",
    "    2 new columns: added_words and removed_words\n",
    "    \"\"\"\n",
    "\n",
    "    get_removed_udf = udf(get_removed_col, types.ArrayType(types.StringType()).simpleString())\n",
    "    df = df.withColumn('removed_words', get_removed_udf(df.difference))\n",
    "    \n",
    "    get_added_udf = udf(get_added_col, types.ArrayType(types.StringType()).simpleString())\n",
    "    df = df.withColumn('added_words', get_added_udf(df.difference))\n",
    "    \n",
    "    df = df.drop('difference')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data and look at class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data as spark dataframe\n",
    "wiki_df = get_wiki_df(path=\"../data/subset/*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label distribution when using all data: \n",
    "  - safe: 30333\n",
    "  - unsafe: 4136\n",
    "  - vandal: 270"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dist = get_label_count(wiki_df)\n",
    "plt.figure(figsize=(8,4))\n",
    "ax = class_dist.select(\"*\").toPandas().plot.bar(x='label', y='count(1)', rot=0, legend=False)\n",
    "ax.set_xlabel(\"Classes\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "# save figure\n",
    "plt.savefig('../output/figures/fig1.png', dpi=DPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get clean dataframe (cleaning of comment, title_page, name_user):\n",
    "clean_df = get_clean_df(wiki_df)\n",
    "\n",
    "# In order to get the actual difference column\n",
    "df_with_difference = get_difference_column(clean_df)\n",
    "\n",
    "# Example of a difference column of the first of 20 instances:\n",
    "# difference column is in the form REMOVED PART |SEPERATIONLINEADDEDREMOVED| ADDED PART\n",
    "df_with_difference.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 columns: removed and added with space in new function. + Clean text_old and text_new\n",
    "# Split difference column into column 'removed' and column 'added'\n",
    "\n",
    "final_df = split_difference_into_removed_added(df_with_difference)\n",
    "final_df.show(10)\n",
    "final_df.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
