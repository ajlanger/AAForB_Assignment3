{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has following goals:\n",
    "- Get difference between old and new wiki page\n",
    "- Cleaning (lower cases, removing stopwords)\n",
    "- Lemmatizing\n",
    "- TF-IDF\n",
    "- Feature engineering\n",
    "- ... from: The acquired training data AND future incoming streaming instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import libraries\n",
    "from pyspark.streaming import StreamingContext\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "from threading import Thread\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, struct, array, col, lit, regexp_replace, lower\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from difflib import unified_diff\n",
    "import difflib\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from pyspark.sql.functions import monotonically_increasing_id \n",
    "import itertools\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Get streaming instances\n",
    "This section is only for the purpose to set up and do intermediate checks of the pipeline with live data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This only serves as trial to test my functions on live incoming instances\n",
    "class StreamingThread(Thread):\n",
    "    def __init__(self, ssc):\n",
    "        Thread.__init__(self)\n",
    "        self.ssc = ssc\n",
    "    # Start stream\n",
    "    def run(self):\n",
    "        ssc.start()\n",
    "        ssc.awaitTermination()\n",
    "    # Stop stream\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)\n",
    "\n",
    "# ssc = StreamingContext(sc, 10) # Every 10 seconds, construct a mini-batch of RDDs\n",
    "# lines = ssc.socketTextStream(\"seppe.net\", 7778)\n",
    "# lines.pprint()\n",
    "# ssc_t = StreamingThread(ssc)\n",
    "# ssc_t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ssc_t.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load in the collected data\n",
    "This part only serves to load in our collected data: to then experiment and perform featurization, TF-IDF and construct classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Spark dataframe --> various sites state that a dataframe is better than a DDR for textual data\n",
    "def get_wiki_df():\n",
    "    # The path can be either a single text file or a directory storing text files. \n",
    "    # It's possible you'll need to change path to your own directory\n",
    "    path = \"../../data/*\"\n",
    "    wiki_df = spark.read.json(path)\n",
    "\n",
    "    # Uncomment if you want its schema\n",
    "    # wiki_df.printSchema()\n",
    "    return wiki_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Check the amount of label counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_count(wiki_df):\n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "    Last result (so you don't need to run it each time) \n",
    "    -> safe: 30333, unsafe: 4136, vandal: 270\n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "    # Creates a temporary view using the wiki DataFrame\n",
    "    wiki_df.createOrReplaceTempView(\"wikidata\")                            \n",
    "                                                                                  \n",
    "    # Check the amount of safes/unsafes/vandals                                    \n",
    "    label_df = spark.sql(\"SELECT label, count(*) FROM wikidata GROUP BY label\")    \n",
    "    return label_df.show()                                                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helperfunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "# HELPER FUNCTIONS\n",
    "######################################################################################\n",
    "\n",
    "def show_X_rows(df, x): #--------------------------------> Shows first X number of spark dataframe rows\n",
    "    # Convert list to RDD\n",
    "    rdd = spark.sparkContext.parallelize(df.take(x))\n",
    "\n",
    "    # Create data frame\n",
    "    df_temp = spark.createDataFrame(rdd)\n",
    "    return df_temp.show()\n",
    "\n",
    "def flatten(nested_list):\n",
    "    flat_list = []\n",
    "    for sublist in nested_list:\n",
    "        if type(sublist) == list:\n",
    "            for item in sublist:\n",
    "                flat_list.append(item)\n",
    "        else:\n",
    "            flat_list.append(sublist)\n",
    "    return flat_list\n",
    "\n",
    "######################################################################################\n",
    "# FUNCTIONS TO GET DIFFERENCE BETWEEN OLD AND NEW TEXT\n",
    "######################################################################################\n",
    "\n",
    "# Get edited part of wiki page (credits to the professor)\n",
    "def get_diff_1(old, new):\n",
    "    return '\\n'.join([ l for l in unified_diff(old.split('\\n'), new.split('\\n')) if l.startswith('+') or l.startswith('-') ])\n",
    "\n",
    "# The difference function that will take a very long time to compute for all instances\n",
    "def get_diff_2(old, new):\n",
    "    deleted_words = []\n",
    "    added_words = []\n",
    "    temp_i = -100\n",
    "    new_word = ''\n",
    "    status = 'none'\n",
    "    for i, s in enumerate(difflib.ndiff(old, new)):\n",
    "        if s[0] == ' ':\n",
    "            if status == 'adding':\n",
    "                added_words.append(new_word)\n",
    "                new_word = ''\n",
    "            elif status == 'deleting':\n",
    "                deleted_words.append(new_word)\n",
    "                new_word = ''\n",
    "            status = 'none'\n",
    "            continue\n",
    "        elif s[0] == '-':\n",
    "            if status != 'deleting':\n",
    "                added_words.append(new_word)\n",
    "                new_word = ''\n",
    "                status = 'deleting'\n",
    "            new_word += s[-1]\n",
    "        elif s[0] == '+':\n",
    "            if status != 'adding':\n",
    "                deleted_words.append(new_word)\n",
    "                new_word = ''\n",
    "                status = 'adding'\n",
    "            new_word += s[-1]\n",
    "    if new_word != '' and status == 'deleting':\n",
    "        deleted_words.append(new_word)\n",
    "    elif new_word != '' and status == 'adding':\n",
    "        added_words.append(new_word)\n",
    "    return {'added': added_words, 'deleted': deleted_words}\n",
    "\n",
    "def get_deletions(del_add):\n",
    "    return del_add['deleted']\n",
    "\n",
    "def get_additions(del_add):\n",
    "    return del_add['added']\n",
    "\n",
    "def extract_differences(str_old, str_new):\n",
    "    try:\n",
    "        # Clean old and new wiki pages from html characters\n",
    "        str_old, str_new = cleanhtml(str_old), cleanhtml(str_new)\n",
    "\n",
    "        # Get big chunks of altered fragments\n",
    "        diff1 = get_diff_1(str_old, str_new)\n",
    "\n",
    "\n",
    "        # Clean the differences from unwanted characters to get only words\n",
    "        diff2 = []\n",
    "        for txt in diff1.split('\\n'):\n",
    "            diff2.append(''.join([x for x in txt if x in string.ascii_letters + '\\'-+ 1234567890']).lower())\n",
    "\n",
    "        # Find chunks that are related ('+' vs '-') else append them to a seperate list that doesn't need to get processed in the next part\n",
    "        fully_added, fully_removed, partly_new, partly_old = find_similar_chunks(diff2)\n",
    "\n",
    "        # Get the individual fragments that were added or deleted\n",
    "        for i in range(0,len(partly_new)):\n",
    "            difference = get_diff_2(partly_new[i], partly_old[i])\n",
    "            for el in difference['added']:\n",
    "                fully_added.append(el)\n",
    "            for el in difference['deleted']:\n",
    "                fully_removed.append(el)\n",
    "\n",
    "        fully_removed = flatten(fully_removed)\n",
    "        fully_added = flatten(fully_added)\n",
    "        while '' in fully_added:\n",
    "            fully_added.remove('')\n",
    "        while '+' in fully_added:\n",
    "            fully_added.remove('+')\n",
    "        while '-' in fully_added:\n",
    "            fully_added.remove('-')\n",
    "\n",
    "        while '' in fully_removed:\n",
    "            fully_removed.remove('')\n",
    "        while '+' in fully_removed:\n",
    "            fully_removed.remove('+')\n",
    "        while '-' in fully_removed:\n",
    "            fully_removed.remove('-')\n",
    "\n",
    "        # Original output_lib was a dictionary\n",
    "        #output_lib = {'added': fully_added, 'removed': fully_removed}\n",
    "\n",
    "        output_str = ''\n",
    "        # However, now we'd like to have the output in a list with seperator \"|SEPERATIONLINEADDEDREMOVED|\"\n",
    "        for word in fully_added:\n",
    "            output_str += '{} '.format(word)\n",
    "\n",
    "        output_str += ' |SEPERATIONLINEADDEDREMOVED|'\n",
    "\n",
    "        for word in fully_removed:\n",
    "            output_str += ' {}'.format(word)\n",
    "\n",
    "        full_difference = []\n",
    "\n",
    "        return output_str\n",
    "    except:\n",
    "        return 'error_would_occur'\n",
    "\n",
    "#     for i in output_lib['added']:\n",
    "#         full_difference.append(i)\n",
    "#     for i in output_lib['removed']:\n",
    "#         full_difference.append(i)\n",
    "\n",
    "#     return full_difference\n",
    "\n",
    "def find_similar_chunks(chunk_list):\n",
    "    fully_added, fully_removed, partly_removed, partly_added = [], [], [], []\n",
    "    # Remove '', ---, +++\n",
    "    while '' in chunk_list:\n",
    "        chunk_list.remove('')\n",
    "    while '+++ ' in chunk_list:\n",
    "        chunk_list.remove('+++ ')\n",
    "    while '--- ' in chunk_list:\n",
    "        chunk_list.remove('--- ')\n",
    "    for chunk in chunk_list:\n",
    "        try:\n",
    "            if chunk[3] not in string.ascii_letters + '\\'-+ 1234567890':\n",
    "                chunk_list.remove(chunk)\n",
    "            else:\n",
    "                continue\n",
    "        except:\n",
    "            chunk_list.remove(chunk)\n",
    "\n",
    "    for a, b in itertools.combinations(chunk_list, 2):\n",
    "        if a[1:30] == b[1:30]:\n",
    "            if a[0] == '+':\n",
    "                partly_added.append(a)\n",
    "                partly_removed.append(b)\n",
    "                chunk_list.remove(a)\n",
    "                chunk_list.remove(b)\n",
    "            else:\n",
    "                partly_added.append(b)\n",
    "                partly_removed.append(a)\n",
    "                chunk_list.remove(a)\n",
    "                chunk_list.remove(b)\n",
    "    for rest in chunk_list:\n",
    "        if rest[0] == '-':\n",
    "            fully_removed.append(rest[1:].split(' '))\n",
    "        elif rest[0] == '+':\n",
    "            fully_added.append(rest[1:].split(' '))\n",
    "    return fully_added, fully_removed, partly_removed, partly_added # The partly removed and partly added ones will be used as input to determine difference\n",
    "\n",
    "######################################################################################\n",
    "# FUNCTIONS TO CLEAN COLUMNS 'comment', 'title', 'user', 'text_old', 'text_new'\n",
    "######################################################################################\n",
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    \n",
    "    # Remove urls\n",
    "    cleantext_no_urls = re.sub(r\"http\\S+\", \"\", cleantext)\n",
    "    return cleantext_no_urls\n",
    "\n",
    "def cleantext(raw):\n",
    "    # If there's nothing in raw, return 'EMPTY'\n",
    "    for item in 'azertyuiopmlkjhgfdsqnbvcxw,;:=ùµ$^)àç!è§(é&1234567890\\\"\\')|@#]}{[^-_':\n",
    "        if item in raw:\n",
    "    \n",
    "            cleanr = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "            cleantext = re.sub(cleanr, '', raw)\n",
    "\n",
    "            # Remove urls\n",
    "            cleantext_no_urls = re.sub(r\"http\\S+\", \"\", cleantext)\n",
    "            return ''.join([x for x in cleantext_no_urls if x in string.ascii_letters + '\\'-+ 1234567890']).lower()\n",
    "    return 'empty'\n",
    "\n",
    "def get_clean_df(df):\n",
    "    # Remove url_page column\n",
    "    df_without_url = df.drop('url_page')\n",
    "\n",
    "    # Cleaning comment, title_page and name_user\n",
    "    clean_udf = udf(cleantext, StringType())\n",
    "    df_without_url = df_without_url.withColumn('clean_comment', clean_udf(df_without_url.comment)).drop('comment')\n",
    "    df_without_url = df_without_url.withColumn('clean_title_page', clean_udf(df_without_url.title_page)).drop('title_page')\n",
    "    df_without_url = df_without_url.withColumn('clean_name_user', clean_udf(df_without_url.name_user)).drop('name_user')\n",
    "    \n",
    "    # Clean the old and new text columns\n",
    "    df_without_url = df_without_url.withColumn('clean_old_text', clean_udf(df_without_url.text_old))\n",
    "    df_without_url = df_without_url.withColumn('clean_new_text', clean_udf(df_without_url.text_new))\n",
    "    \n",
    "    data = df_without_url.select(col(\"label\"), col(\"clean_comment\").alias(\"comment\"), col(\"clean_title_page\").alias(\"title_page\"), col(\"clean_name_user\").alias(\"name_user\"), col(\"text_old\"), col(\"text_new\"), col(\"clean_old_text\"), col(\"clean_new_text\"))\n",
    "    return data\n",
    "\n",
    "def get_difference_column(df):\n",
    "    difference_udf = udf(extract_differences, StringType())\n",
    "    intermediate_col = df.withColumn('difference', difference_udf(df.text_old, df.text_new))\n",
    "    intermediate_col = intermediate_col.drop('text_old')\n",
    "    intermediate_col = intermediate_col.drop('text_new')\n",
    "    return intermediate_col\n",
    "\n",
    "def paste_words(list_of_words):\n",
    "    return ' '.join([x for x in list_of_words])\n",
    "\n",
    "######################################################################################\n",
    "# FUNCTIONS TO SPIT DIFFERENCE COLUMN INTO ADDED AND REMOVED COLUMN\n",
    "######################################################################################\n",
    "\n",
    "def get_removed_col(col):\n",
    "    for item in 'azertyuiopmlkjhgfdsqnbvcxw,;:=ùµ$^)àç!è§(é&1234567890\\\"\\')|@#]}{[^-_':\n",
    "            if item in col[:col.find('|SEPERATIONLINEADDEDREMOVED|')]:\n",
    "                return col[:col.find('|SEPERATIONLINEADDEDREMOVED|')].split(' ')\n",
    "    return ['empty']\n",
    "\n",
    "def get_added_col(col):\n",
    "    for item in 'azertyuiopmlkjhgfdsqnbvcxw,;:=ùµ$^)àç!è§(é&1234567890\\\"\\')|@#]}{[^-_':\n",
    "            if item in col[col.find('|SEPERATIONLINEADDEDREMOVED|') + len('|SEPERATIONLINEADDEDREMOVED|') + 1:]:\n",
    "                return col[col.find('|SEPERATIONLINEADDEDREMOVED|') + len('|SEPERATIONLINEADDEDREMOVED|') + 1:].split(' ')\n",
    "    return ['empty']\n",
    "\n",
    "def split_difference_into_removed_added(df):\n",
    "    get_removed_udf = udf(get_removed_col, StringType())\n",
    "    df = df.withColumn('removed_words', get_removed_udf(df.difference))\n",
    "    \n",
    "    get_added_udf = udf(get_added_col, StringType())\n",
    "    df = df.withColumn('added_words', get_added_udf(df.difference))\n",
    "    \n",
    "    df = df.drop('difference')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data as spark dataframe\n",
    "#wiki_df = get_wiki_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get clean dataframe (cleaning of comment, title_page, name_user):\n",
    "#clean_df = get_clean_df(wiki_df)\n",
    "\n",
    "# In order to get the actual difference column\n",
    "#df_with_difference = get_difference_column(clean_df)\n",
    "\n",
    "# Example of a difference column of the first of 20 instances:\n",
    "# difference column is in the form REMOVED PART |SEPERATIONLINEADDEDREMOVED| ADDED PART\n",
    "# df_with_difference.show(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|label|             comment|          title_page|         name_user|      clean_old_text|      clean_new_text|       removed_words|         added_words|\n",
      "+-----+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| safe|   fixed cite errors|timeline of the 2...|         john b123|short description...|short description...|[last, websitewww...|[cnbeta, lastcnn,...|\n",
      "| safe|duplicate word re...|timeline of the 2...|           arjayay|short description...|short description...|             [empty]|             [the, ]|\n",
      "| safe|         23 february|timeline of the 2...|    reddyhakky1998|see alsotimeline ...|see alsotimeline ...|    [february, , , ]|        [february, ]|\n",
      "| safe|4 februaryadded w...|timeline of the 2...|sebastianrueckoldt|see alsotimeline ...|see alsotimeline ...|[world, health, o...|             [empty]|\n",
      "+-----+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 columns: removed and added with space in new function. + Clean text_old and text_new\n",
    "# Split difference column into column 'removed' and column 'added'\n",
    "#final_df = split_difference_into_removed_added(df_with_difference)\n",
    "\n",
    "#final_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| label|             comment|          title_page|           name_user|      clean_old_text|      clean_new_text|       removed_words|         added_words|\n",
      "+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|vandal|                 add|            carnival|         75187130123|aboutthe celebrat...|aboutthe celebrat...|     eating grasss  |               empty|\n",
      "|vandal|               empty|  2011 england riots|           922340146|short description...|short description...|title 2012 oli wa...|title 2011 englan...|\n",
      "|vandal|               empty|history of united...|         20913123780|furtherforeign po...|furtherforeign po...|    i got to go p...|               empty|\n",
      "|vandal|               empty|richard iii of en...|2a02c7f485dac0095...|redirectrichard i...|redirectrichard i...|issue-link ''ital...|          issue-link|\n",
      "|vandal|               empty|               gulag|2a01e34ec06e8c030...|other usesshort d...|other usesshort d...|caca proutmacron ...|               empty|\n",
      "|vandal|            behavior|    great horned owl|         liljimmy911|overly detailedda...|the great horned ...|  overly detailed...|overly detailedda...|\n",
      "|vandal|       assez content|   the little prince|     theredvelvetkid|aboutthe novellat...|aboutthe novellat...|wanna be my boyfr...|               empty|\n",
      "|vandal|              africa|list of most popu...|          1736688210|short description...|short description...|africa is a count...|              africa|\n",
      "|vandal|               empty|remilitarization ...|26001014b02c46b43...|filerhinelandjpgt...|my ham sandwich w...|my ham sandwich w...|filerhinelandjpgt...|\n",
      "|vandal|season 3 mission ...|list of super win...|             6745585|short description...|short description...|season 4 super ch...|               empty|\n",
      "|vandal|  compilation albums|     rem discography|           771671710|short description...|short description...|formats cd digita...|formats cd lp dig...|\n",
      "|vandal|     series overview|list of rwby epis...|2605a601ac996a00b...|displaytitlelist ...|displaytitlelist ...|end6 start date i...|end6 start date20...|\n",
      "|vandal|i have changed wh...|     arabic alphabet|2a0023c61a0ec300b...|short description...|short description...|the arabbebebehe'...|the '''arabic alp...|\n",
      "|vandal|       studio albums|    tyga discography|        191113185143|use mdy datesdate...|use mdy datesdate...|    error_would_occu|               empty|\n",
      "|vandal|               empty|list of trigonome...|      princess vixen|short description...|short description...|+these identities...|hese identities a...|\n",
      "|vandal|               empty|      the lego movie|           819964207|redirectlego movi...|redirectlego movi...|chris pratt i've ...|chris pratt will ...|\n",
      "|vandal|me and my family ...|           breakfast|          4111389225|other usesfileamc...|other usesfileamc...|me and my family ...|               empty|\n",
      "|vandal|               empty|world war i repar...|         10818214123|paris peace confe...|paris peace confe...|my dog is sooooo ...|               empty|\n",
      "|vandal|              albums|list of adventure...|           130453773|italic titlestrin...|italic titlestrin...|    2 'hello '''''  |2 ''grins grabber...|\n",
      "|vandal|               empty|foreign relations...|240520438d11b4158...|use dmy datesdate...|use dmy datesdate...| all this informa...|               empty|\n",
      "|vandal|      regular season|2020 nrl season r...|           141138111|main2020 nrl seas...|main2020 nrl seas...|    error_would_occu|               empty|\n",
      "|vandal|               empty|voyages of christ...|26012418d00f980d8...|use mdy datesdate...|use mdy datesdate...|n columbus was a ...|               empty|\n",
      "|vandal|t c bear minnesot...|list of major lea...|          7137216145|short description...|short description...|         and i oop  |               empty|\n",
      "|vandal|       personal life|       john williams|2a02c7fa8961100e4...|aboutthe composer...|aboutthe composer...|his favourite hob...|cite weburl bosto...|\n",
      "|vandal|               empty|  egyptian mythology|        tardisfan 12|see alsoancient e...|arded as myths in...|arded as myths in...|see alsoancient e...|\n",
      "|vandal|               empty|       henri matisse|           822483187|short description...|dis is my new hom...|dis is my new hom...|short description...|\n",
      "|vandal|               empty|south georgia and...|2600170023c07570d...|short description...|short description...|year along with a...|postage stamps ma...|\n",
      "|vandal|               empty|        rta meilutyt|           846879166|short description...|short description...|everyone likes her  |               empty|\n",
      "|vandal|         performance|national health s...|  secretspypopstar28|redirectnhsforthe...|redirectnhsforthe...|as of march 2020 ...|under pressure in...|\n",
      "|vandal|               death|richard ii of eng...|2a02c7fbc3a3a0069...|short description...|short description...| i just want to k...|               empty|\n",
      "|vandal|               empty|zoom video commun...|           116102028|redirectzoomshort...|redirectzoomshort...|they are scumbags...|               empty|\n",
      "|vandal|just there where ...|       equestrianism|             soboley|other usesequestr...|other usesequestr...|''horses are the ...|'''equestrianism'...|\n",
      "|vandal|               empty|2020 coronavirus ...|260124453803d30f9...|short description...|short description...| the coronavirus ...|               empty|\n",
      "|vandal|i added a sentenc...| alien hand syndrome|          amahamaama|short description...|original research...|a distinguishable...|short description...|\n",
      "|vandal|               empty|         persian cat|         11220172232|use mdy datesdate...|use mdy datesdate...|             hello  |               empty|\n",
      "|vandal|               empty|            d ghetto|           821018411|short description...|short description...| the jews died a ...|               empty|\n",
      "|vandal|               empty|          outlookcom|          6622925171|aboutthe email an...|aboutthe email an...|logo microsoft of...|logo microsoft of...|\n",
      "|vandal|               empty|     white hart lane|2a0023c7b101d3011...|short description...|short description...|if you look close...|               empty|\n",
      "|vandal|     life and career|    maria montessori|        208107210253|short description...|short description...|life and careers ...|     life and career|\n",
      "|vandal|      to add content|       edward jenner|2a02c7f30083500c5...|forthe new zealan...|forthe new zealan...|    error_would_occu|               empty|\n",
      "|vandal|               empty|          david koch|2001d08d2bc38a4c3...|aboutthe american...|aboutthe american...|' dont believe wi...|'''david hamilton...|\n",
      "|vandal|              delete|second battle of ...|260017007621f3004...|use mdy datesdate...|pope was relieved...|    error_would_occu|               empty|\n",
      "|vandal|pronunciation of ...|    johannes vermeer|          8484203165|short description...|short description...|ee e i do not kno...|there m e a disti...|\n",
      "|vandal|               empty|succession to the...|        169239192177|pp-pc1use dmy dat...|pp-pc1use dmy dat...| ugonna will be q...|               empty|\n",
      "|vandal|          background|             comrade|          2039948181|short description...|short description...|yo whats up ghaus...|          background|\n",
      "|vandal|                   c|list of incurable...|         10321014027|short description...|short description...|+coronavirus  it ...|rona irus  it is ...|\n",
      "|vandal|               empty|president of paki...|           860180254|short description...|short description...|bum bum  d poo po...|              pakist|\n",
      "|vandal|               empty|president of paki...|           860180254|short description...|short description...|bum bum  d poo po...|              pakist|\n",
      "|vandal|               empty|list of national ...|           275522170|short description...|short description...|bye bey  bye bye ...|               empty|\n",
      "|vandal|               empty|   continental drift|2a0281099c805afa2...|short description...|short description...|although wegener'...|although wegener'...|\n",
      "|vandal|   i just updated it|          atif aslam|       starstruck 12|engvarbdatenovemb...|engvarbdatenovemb...| also known as tu...|               empty|\n",
      "|vandal|                life|     antonio vivaldi|           681924032|redirectvivaldish...|redirectvivaldish...|please help this ...|           childhood|\n",
      "|vandal|                 628|timeline of astro...|200156a74d8a100d9...|notoc 3114 bc may...|notoc 3114 bc may...|             i eat  |               empty|\n",
      "|vandal|          the asylum|    the starry night|          6531162197|redirectstarry ni...|redirectstarry ni...|      hey whats up  |               empty|\n",
      "|vandal|          early life| fanny blankers-koen|2a0023c5b4cdae00c...|use dmy datesdate...|use dmy datesdate...|uck my divk namei...|               empty|\n",
      "|vandal|   joan jett version|i love rock 'n' roll|           904039202|other usesshort d...|other usesshort d...|length 255 lp ver...|length 255 lp ver...|\n",
      "|vandal|2019 coronavirus ...|public health eme...|          8023348236|short description...|short description...|201920 coronaviru...|2019 coronavirus ...|\n",
      "|vandal|appropriation of ...|   rainbow flag lgbt|          7421459253|short description...|short description...| homosexuales app...|               empty|\n",
      "|vandal|                main|  greenhouse academy|           581823116|short description...|short description...|        i love bts  |main ariel mortma...|\n",
      "|vandal|giving more conte...|       matthew lloyd|         12210522333|short description...|short description...|wh  ha  a reputat...|           ll y s af|\n",
      "|vandal|               empty|     steve mcgarrett|           847745226|use mdy datesdate...|use mdy datesdate...|  ref stiff es gey  |               empty|\n",
      "|vandal|           satirists|list of british c...|          3115114145|engvarbdatedecemb...|engvarbdatedecemb...|       colend halo  |              colend|\n",
      "|vandal|          references|               laity|         81103177108|short description...|short description...|     reflist2 cool  |            reflist2|\n",
      "|vandal|                  ha|penrith new south...|          1201543632|forthe local gove...|forthe local gove...|'''penrith is a h...|type suburb north...|\n",
      "|vandal|               empty|        dan crenshaw|          6423125116|use mdy datesdate...|use mdy datesdate...|hes also a nazi a...|n he r n for c re...|\n",
      "|vandal|             details|  hoover institution|           722252967|aboutthe american...|aboutthe american...|label1 research o...|label1 research l...|\n",
      "|vandal|               empty|      jason reynolds|260088035c0080530...|pp-pc1use mdy dat...|pp-pc1use mdy dat...|     he is awesome  |               empty|\n",
      "|vandal|      national style| football in belgium|          6912419440|infobox sport ove...|infobox sport ove...| did you know tha...|               empty|\n",
      "|vandal|i added context t...| omnipotence paradox|          7315417169|fileaverroescolor...|fileaverroescolor...| ok lk hart added...|               empty|\n",
      "|vandal|            el greco|  spanish golden age|           371186200|short description...|short description...|                xd  |               empty|\n",
      "|vandal|               empty|              douala|           129076241|other usesduala d...|other usesduala d...|'''douala taku re...|'''douala''' lang...|\n",
      "|vandal|               empty|       pandora's box|           722162137|aboutthe mytholog...|aboutthe mytholog...| it was mistransl...|               empty|\n",
      "|vandal|death by falling ...|    death by coconut|          8525010027|use dmy datesdate...|use dmy datesdate...|i like an al 69 i...|               empty|\n",
      "|vandal|this content shou...|              sunday|       jippywoahwoah|short description...|short description...|the name sunday t...|monday shall be i...|\n",
      "|vandal|           being gay|      roald amundsen|         75166194186|redirectamundsenp...|redirectamundsenp...|      yes very gay  |early life amunds...|\n",
      "|vandal|       added content|              aeneas|           lalagwood|short description...|short description...|hes also known fo...|               empty|\n",
      "|vandal|               empty|universally uniqu...|         17813288221|use dmy datesdate...|use dmy datesdate...|aay universallyay...|a '''universally ...|\n",
      "|vandal|               empty|            bowsette|          9993175128|short description...|short description...|+journalists took...|ourn lists took n...|\n",
      "|vandal|           geography|             liguria|          9314428126|short description...|short description...|+liguria is borde...|liguri  is bord r...|\n",
      "|vandal|           etymology|              yishun|24063003206f35e0d...|engvarbdatejuly 2...|engvarbdatejuly 2...| that's pretty gay  |               empty|\n",
      "|vandal|               empty|          deviantart|               empty|short description...|short description...|logo founders sco...|logo deviantart l...|\n",
      "|vandal|                  pp|             amniote|        turtlestinky|short description...|short description...|'''amniotes''' th...|'''amniotes''' fr...|\n",
      "|vandal|               empty|      james earl ray|              56564b|pp-move-indefuse ...|pp-move-indefuse ...|and also f this g...|               empty|\n",
      "|vandal|               story|       lady jewelpet|          1199522159|more citations ne...|more citations ne...|+momona is an ord...|momona is an ordi...|\n",
      "|vandal|       added content|      jewish deicide|         59100196186|short description...|short description...|'''jewish deicide...|'''jewish deicide...|\n",
      "|vandal|               empty|leesburg high sch...|            81924831|use mdy datesdate...|use mdy datesdate...|principal mr clea...|principal michael...|\n",
      "|vandal|               empty| xu xin table tennis|2a02c7dd8197c00d5...|short description...|short description...|residence we dont...|           residence|\n",
      "|vandal|               empty|east asian community|200144504691ba004...|'''east asian com...|'''east asian com...|               empty|               empty|\n",
      "|vandal|               empty|          golden age|         10322548177|abouta period in ...|abouta period in ...|you do sex with g...|see alsogolden ag...|\n",
      "|vandal|changed the color...|2020 south korean...|   someguywhosnormal|use dmy datesdate...|use dmy datesdate...|stylebackground-c...|stylebackground-c...|\n",
      "|vandal|     gave a heads-up|the adventures of...|            anaiyoop|short description...|you fool you abso...|you fool you abso...|short description...|\n",
      "|vandal|              family|  benot assou-ekotto|         matherton99|redirectassou-eko...|redirectassou-eko...| he also has a yo...|               empty|\n",
      "|vandal|          background|        rory gilmore|26012c049006a0092...|infobox character...|infobox character...|  my name is ciara  |               empty|\n",
      "|vandal|references to pas...|blood money break...|2a02810a8940103fc...|short description...|short description...|     and who cares  |               empty|\n",
      "|vandal|         by adding 3|spain in the midd...|2a02aa7402ae9c591...|history of spaini...|history of spaini...|   3 you wikipedia  |               empty|\n",
      "|vandal|early life and ca...|gerard archbishop...|    rihan ahmad khan|botsdenyinterneta...|rihan ahmad khan ...|rihan ahmad khan ...|botsdenyinterneta...|\n",
      "|vandal|               empty|         jordan ayew|          2132052413|engvarbdatemay 20...|engvarbdatemay 20...| better than any ...|               empty|\n",
      "|vandal|   characterizations|equilateral triangle|         20725539154|redirectequilater...|redirectequilater...|               empty|               empty|\n",
      "|vandal|               empty|     kinder surprise|         72189128188|short description...|short description...|     good job eggs  |               empty|\n",
      "|vandal|partiesedited out...|list of political...|          2129815253|short description...|short description...|    error_would_occu|               empty|\n",
      "+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# final_df.filter(df_removed_added_colls.label == 'vandal').show(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
