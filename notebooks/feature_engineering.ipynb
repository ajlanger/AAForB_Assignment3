{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering on Text Data\n",
    "\n",
    "In this notebook, we calculate features on data streamed from seppe.net in Preprocessing.ipynb. We calculate the following features on the data and columns in the extracted wiki_df dataframe:\n",
    "\n",
    "- TF-IDF: Term Frequency - Inverse Document Frequency matrix is a feature which measures the occurrence of words normalized by their overall occurrence in the entire document corpus. We use this on the raw edits applied to each Wikipedia article to help gather features as to which words and terms in overall edits may lead to vandal edits or otherwise.\n",
    "- LDA: Latent Dirichlet Analysis is a technique used in automated topic discovery. We use this on the overall Wiki text before edit to discover the original topic of the piece. The reason for using this feature is that some topics may be more susceptible to vandalism than others, such as political articles, for example.\n",
    "- Leichtenstein Distance: This is used again on the raw edits to quantify the size of the edit. Usually large edits might correspond to large erasures or changes in a document text indicating vandalism and censoring of data from the public."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the feature transformation classes for doing TF-IDF \n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover, CountVectorizer, IDF, NGram\n",
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run \"preprocessing.ipynb\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "wiki_df = get_wiki_df()\n",
    "\n",
    "clean_df = get_clean_df(wiki_df)\n",
    "\n",
    "# In order to get the actual difference column\n",
    "df_with_difference = get_difference_column(clean_df)\n",
    "\n",
    "final_df = split_difference_into_removed_added(df_with_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stratified_sample(df, fractions, categorical_class=\"label\", random_state = 42):\n",
    "    \"\"\"\n",
    "    This function creates a stratified sample based on thresholds specified on a categorical class\n",
    "    The aim of this is to balance a dataset more evenly by reducing the size of over-prepresented classes.\n",
    "    \n",
    "    Args:\n",
    "        df: pyspark dataframe with data to be stratified sampled\n",
    "        fractions: a dictionary of fractions for each category in the categorical variable\n",
    "        categorical_class: the variable on which to perform stratified sampling\n",
    "        random_state: default = 42. Set the seed for reproducibility\n",
    "    Returns:\n",
    "        df: a pyspark dataframe which has been stratified sampled based on the above criteria.\n",
    "    \"\"\"\n",
    "    auto_fractions = df.select(\"{}\".format(categorical_class)).distinct().withColumn(\"fraction\", lit(1.0)).rdd.collectAsMap()\n",
    "    #fractions = {'safe': 0.1, 'unsafe': 1.0, 'vandal':1.0}\n",
    "    # override default 1.0 non-samples with classes which need to be subsampled\n",
    "    for frac in fractions.items():\n",
    "        key = frac[0]\n",
    "        frac_value = frac[1]\n",
    "        auto_fractions[key] = frac_value\n",
    "    \n",
    "    seed = random_state\n",
    "    sampled_df = df.stat.sampleBy(categorical_class, auto_fractions, seed)\n",
    "    return sampled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Features on New and Old Texts\n",
    "\n",
    "Term Frequency - Inverse Document Frequency (TF-IDF) is a technique used to build features out of text documents which have theoretically infinite dimensionality without feature reduction techniques such as this. The term-frequency is the step where we take the tokenized words from the text documents and hash them to a finite feature space. The resulting vectors represent a single document of text. For example, the text 'the brown fox' will hash to a vector of specified length, say 5, such that the result of the hash yields [1,0,2,0,0]. In the case of Spark, the hash used is MurmurHash 3.\n",
    "\n",
    "However, in a large text corpus, some words will be very present (e.g. “the”, “a”, “is”) hence carrying very little meaningful information about the actual contents of the document. If we were to feed the direct count data directly to a classifier those very frequent terms would shadow the frequencies of rarer yet more interesting terms.\n",
    "\n",
    "In order to re-weight the count features into floating point values suitable for usage by a classifier it is very common to incorporate the document frequency of occurrence as a weight or normalization to the term-frequencies mentioned above. Hence, TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfIdf(df, text_col_for_tf_idf, output_tf_idf_col, count_method = 'hash'):\n",
    "    \"\"\" This fucntion takes the text data and converts it into a term frequency-Inverse Document Frequency vector\n",
    "        The steps for this are tokenization of the input string column, stop word removal, feature hashing/count vectorization depending on \n",
    "        the count_method, and inverse document normalization step.\n",
    "        \n",
    "    parameter: \n",
    "        text_col_for_tf_idf: input text column of typ 'string' in Java which is used as input to the tokenization, stop word removal and TF-IDF step\n",
    "        output_tf_idf_col: output column to store the resulting feature\n",
    "        count_method: default = 'hash'. Determines whether to use featuer hashing or counts as the TF step for TF-IDF\n",
    "    returns: dataframe with tf-idf vectors\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Carrying out the Tokenization of the text documents (splitting into words)\n",
    "    tokenizer = Tokenizer(inputCol=text_col_for_tf_idf, outputCol=\"tokenised_text\")\n",
    "    tokensDf = tokenizer.transform(df)\n",
    "    # Carrying out the StopWords Removal for TF-IDF\n",
    "    stopwordsremover=StopWordsRemover(inputCol='tokenised_text',outputCol='words')\n",
    "    swremovedDf= stopwordsremover.transform(tokensDf)\n",
    "\n",
    "    if count_method == 'hash':\n",
    "        # hashing is irreversible whereas counting is \n",
    "        # While applying HashingTF only needs a single pass to the data, applying IDF needs two passes:\n",
    "        # First to compute the IDF vector and second to scale the term frequencies by IDF.\n",
    "        hashingTF = HashingTF(inputCol=\"words\", outputCol=\"tf_features\")\n",
    "        tfDf = hashingTF.transform(swremovedDf)\n",
    "    else:\n",
    "        # Creating Term Frequency Vector for each word\n",
    "        cv=CountVectorizer(inputCol=\"words\", outputCol=\"tf_features\", vocabSize=300, minDF=2.0)\n",
    "        cvModel=cv.fit(swremovedDf)\n",
    "        tfDf=cvModel.transform(swremovedDf)\n",
    "\n",
    "    # Carrying out Inverse Document Frequency on the TF data\n",
    "    # spark.mllib's IDF implementation provides an option for ignoring terms\n",
    "    # which occur in less than a minimum number of documents.\n",
    "    # In such cases, the IDF for these terms is set to 0.\n",
    "    # This feature can be used by passing the minDocFreq value to the IDF constructor.\n",
    "    idf=IDF(inputCol=\"tf_features\", outputCol=\"{}\".format(output_tf_idf_col))\n",
    "    idfModel = idf.fit(tfDf)\n",
    "    tfidfDf = idfModel.transform(tfDf)\n",
    "\n",
    "    tfidfDf.cache().count()\n",
    "\n",
    "    return tfidfDf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance Data using Stratified Sampling\n",
    "\n",
    "We do this to ease the memory usage of the TF-IDF. In any case, the data is highly imbalanced, with a current distribution of:\n",
    "\n",
    "- safe: 30333 (~86%)\n",
    "- unsafe: 4136 (~13.2%)\n",
    "- vandal: 270 (~0.8%)\n",
    "\n",
    "It is better to rebalance this by subsampling the \"safe\" class and keeping the others."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sampled_df = get_stratified_sample(df = final_df, fractions = {'safe': 0.15})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "get_label_count(sampled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate TF-IDF via Spark"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tfidfDf = tfIdf(sampled_df, text_col_for_tf_idf = \"clean_new_text\", output_tf_idf_col = \"new_text_tf_idf_features\")\n",
    "tfidfDf = tfIdf(tfidfDf, text_col_for_tf_idf = \"clean_old_text\", output_tf_idf_col = \"old_text_tf_idf_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram Features on Text Differences (Added/Removed)\n",
    "\n",
    "Here we extract n-gram features from the text differences (text added or removed). The goal is from these simple combinations of words to extract usable features for modelling. Since the words are unordered, an n-gram model is appropriate, as it itself is not necessarily order-preserving in its selection of words.\n",
    "\n",
    "We select $n = 2$ for simplicity of the method. Additionally, we optionally apply feature hashing to the resulting n-grams.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams(df, text_col_for_ngrams, output_col_for_ngrams, n = 2, do_feature_hashing = True):\n",
    "    \"\"\" This fucntion takes a text column and converts it to a (hashed or unhashed) n-gram representation.\n",
    "        The steps are to remove stop words and to run the n-gram, then do optional feature hashing.\n",
    "        \n",
    "    parameter: \n",
    "        text_col_for_ngrams: input text column of typ 'string' in Java which is used as input to the stop word removal and n-gram step\n",
    "        output_col_for_ngrams: output column to store the resulting feature\n",
    "        n: default = 2. Determines the value of n for the n-gram calculation. Example, n = 1 is a unigram of single words.\n",
    "        do_feature_hashing: default = True. Determines whether to use featuer hashing or not\n",
    "    returns: dataframe with n-gram vectors\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ngram = NGram(n=n, inputCol=\"{}\".format(text_col_for_ngrams), outputCol=\"ngrams\")\n",
    "    df = ngram.transform(df)\n",
    "    if do_feature_hashing:\n",
    "        # Carrying out the StopWords Removal for TF-IDF\n",
    "        stopwordsremover=StopWordsRemover(inputCol='ngrams',outputCol='words')\n",
    "        swremovedDf= stopwordsremover.transform(df)\n",
    "        # hashing is irreversible whereas counting is \n",
    "        # While applying HashingTF only needs a single pass to the data, applying IDF needs two passes:\n",
    "        # First to compute the IDF vector and second to scale the term frequencies by IDF.\n",
    "        hashingTF = HashingTF(inputCol=\"words\", outputCol=\"{}\".format(output_col_for_ngrams))\n",
    "        tfDf = hashingTF.transform(swremovedDf)  \n",
    "    return tfDf\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tfidfDf = extract_ngrams(tfidfDf, text_col_for_ngrams = \"removed_words\", output_col_for_ngrams = \"removed_words_ngrams_hash_features\")\n",
    "tfidfDf = extract_ngrams(tfidfDf, text_col_for_ngrams = \"added_words\", output_col_for_ngrams = \"added_words_ngrams_hash_features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Pipeline and Modelling\n",
    "\n",
    "Below you can find a summary of code needed to extract features using these methods"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "wiki_df = get_wiki_df()\n",
    "\n",
    "clean_df = get_clean_df(wiki_df)\n",
    "\n",
    "# In order to get the actual difference column\n",
    "df_with_difference = get_difference_column(clean_df)\n",
    "\n",
    "final_df = split_difference_into_removed_added(df_with_difference)\n",
    "\n",
    "sampled_df = get_stratified_sample(df = final_df, fractions = {'safe': 0.15})\n",
    "\n",
    "tfidfDf = tfIdf(sampled_df, text_col_for_tf_idf = \"clean_new_text\", output_tf_idf_col = \"new_text_tf_idf_features\")\n",
    "tfidfDf = tfIdf(tfidfDf, text_col_for_tf_idf = \"clean_old_text\", output_tf_idf_col = \"old_text_tf_idf_features\")\n",
    "\n",
    "tfidfDf = extract_ngrams(tfidfDf, text_col_for_ngrams = \"removed_words\", output_col_for_ngrams = \"removed_words_ngrams_hash_features\")\n",
    "tfidfDf = extract_ngrams(tfidfDf, text_col_for_ngrams = \"added_words\", output_col_for_ngrams = \"added_words_ngrams_hash_features\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
