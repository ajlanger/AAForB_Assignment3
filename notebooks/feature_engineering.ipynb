{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering on Text Data\n",
    "\n",
    "In this notebook, we calculate features on data streamed from seppe.net in Preprocessing.ipynb. We calculate the following features on the data and columns in the extracted wiki_df dataframe:\n",
    "\n",
    "- TF-IDF: Term Frequency - Inverse Document Frequency matrix is a feature which measures the occurrence of words normalized by their overall occurrence in the entire document corpus. We use this on the raw edits applied to each Wikipedia article to help gather features as to which words and terms in overall edits may lead to vandal edits or otherwise.\n",
    "- LDA: Latent Dirichlet Analysis is a technique used in automated topic discovery. We use this on the overall Wiki text before edit to discover the original topic of the piece. The reason for using this feature is that some topics may be more susceptible to vandalism than others, such as political articles, for example.\n",
    "- Leichtenstein Distance: This is used again on the raw edits to quantify the size of the edit. Usually large edits might correspond to large erasures or changes in a document text indicating vandalism and censoring of data from the public."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the feature transformation classes for doing TF-IDF \n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover, CountVectorizer, IDF, NGram\n",
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "C:\\Users\\Pieter-Jan\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj8AAABICAYAAAAZFJRnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAACa0lEQVR4nO3coYpUcRjG4W92TQabhi3CWrwADSYVtnkDGjQJVoPCYBoE4X8DimB32SyYTjHIBrvYbIIYdEEwyfEGxGHDf45z3ueJwwy8X/sxZ5jFOI5jAQCE2Jl6AADAJokfACCK+AEAoogfACCK+AEAoogfACDKmXVvGIahhmGoqqrWWvdBAAA9LU77Pz87j9702jK5X4/36svhtalndLF357jOvj2cekY3P2/crs+rq1PP6Obis5P69PzD1DO6ufzwUn08ejr1jG6u3H1Zv999m3pGF7vXz9eL16+mntHNg/s36+TrvalndPPj3HE9ef996hndHN268NfXPfYCAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgivgBAKKIHwAgymIcx/FfbxiGoYZhqKqq1tpGRgEA9LL2m5+Dg4NqrVVrrZbL5SY2TWbO9835tir3bTv3ba8531blvrny2AsAiCJ+AIAou6vVanWaD+zv73ea8n+Y831zvq3KfdvOfdtrzrdVuW+O1v7gGQBgTjz2AgCiiB8AIIr4AQCiiB8AIIr4AQCi/AFDZVOIbKU9nwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| label|             comment|          title_page|           name_user|      clean_old_text|      clean_new_text|          difference|\n",
      "+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  safe|            map date|201920 coronaviru...|       raphal dunant|redirectcoronavir...|redirectcoronavir...|4  |SEPERATIONLIN...|\n",
      "|  safe|       typo viawpjwb|yemeni civil war ...|        alistair1978|other usesyemen w...|other usesyemen w...|since the mid-200...|\n",
      "|unsafe|               empty|                2019|         annettespry|pp-protectedsmall...|pp-protectedsmall...|cameron boyce ame...|\n",
      "|  safe|2327 marchbroke p...|2020 stock market...|commonknowledgecr...|pp-vandalismsmall...|pp-vandalismsmall...|the bank of japan...|\n",
      "|  safe|               empty|united states men...|              phikia|filetschroederjpg...|filetschroederjpg...|goalkeepers with ...|\n",
      "+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| label|             comment|          title_page|           name_user|      clean_old_text|      clean_new_text|       removed_words|         added_words|\n",
      "+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  safe|            map date|201920 coronaviru...|       raphal dunant|redirectcoronavir...|redirectcoronavir...|             [4, , ]|                [33]|\n",
      "|  safe|       typo viawpjwb|yemeni civil war ...|        alistair1978|other usesyemen w...|other usesyemen w...|[since, the, mid-...|[since, the, mid-...|\n",
      "|unsafe|               empty|                2019|         annettespry|pp-protectedsmall...|pp-protectedsmall...|[cameron, boyce, ...|             [empty]|\n",
      "|  safe|2327 marchbroke p...|2020 stock market...|commonknowledgecr...|pp-vandalismsmall...|pp-vandalismsmall...|[the, bank, of, j...|[, the, bank, of,...|\n",
      "|  safe|               empty|united states men...|              phikia|filetschroederjpg...|filetschroederjpg...|[goalkeepers, wit...|[goalkeepers, wit...|\n",
      "|  safe|             history|2020 coronavirus ...|          wikisaurus|pp-move-indefsmal...|pp-move-indefsmal...|             [empty]|             [empty]|\n",
      "|  safe|date formats perm...|       boris johnson|             alarics|redirectbojoother...|redirectbojoother...|[use, dmy, datesd...|[use, dmy, datesd...|\n",
      "|unsafe|               empty|         axis powers|26046000151382358...|short description...|short description...|[mainfascist, ita...|[mainkingdom, of,...|\n",
      "|  safe|               empty|united states men...|              phikia|this article cont...|this article cont...|[ma, ch, 30, 20, ...|[octobe, 5, 19, p...|\n",
      "|unsafe|               empty|      ottoman empire|          1062085273|pp-pc1short descr...|pp-pc1short descr...|[commonname, otto...|[commonname, otto...|\n",
      "+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run \"preprocessing.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and clean data: set path correctly \n",
    "wiki_df = get_wiki_df(path=\"../data/subset/*\")\n",
    "clean_df = get_clean_df(wiki_df)\n",
    "\n",
    "# In order to get the actual difference column\n",
    "df_with_difference = get_difference_column(clean_df)\n",
    "final_df = split_difference_into_removed_added(df_with_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stratified_sample(df, fractions, categorical_class=\"label\", random_state = 42):\n",
    "    \"\"\"\n",
    "    This function creates a stratified sample based on thresholds specified on a categorical class\n",
    "    The aim of this is to balance a dataset more evenly by reducing the size of over-prepresented classes.\n",
    "    \n",
    "    Args:\n",
    "        df: pyspark dataframe with data to be stratified sampled\n",
    "        fractions: a dictionary of fractions for each category in the categorical variable\n",
    "        categorical_class: the variable on which to perform stratified sampling\n",
    "        random_state: default = 42. Set the seed for reproducibility\n",
    "    Returns:\n",
    "        df: a pyspark dataframe which has been stratified sampled based on the above criteria.\n",
    "    \"\"\"\n",
    "    auto_fractions = df.select(\"{}\".format(categorical_class)).distinct().withColumn(\"fraction\", lit(1.0)).rdd.collectAsMap()\n",
    "    #fractions = {'safe': 0.1, 'unsafe': 1.0, 'vandal':1.0}\n",
    "    # override default 1.0 non-samples with classes which need to be subsampled\n",
    "    for frac in fractions.items():\n",
    "        key = frac[0]\n",
    "        frac_value = frac[1]\n",
    "        auto_fractions[key] = frac_value\n",
    "    \n",
    "    seed = random_state\n",
    "    sampled_df = df.stat.sampleBy(categorical_class, auto_fractions, seed)\n",
    "    return sampled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Features on New and Old Texts\n",
    "\n",
    "Term Frequency - Inverse Document Frequency (TF-IDF) is a technique used to build features out of text documents which have theoretically infinite dimensionality without feature reduction techniques such as this. The term-frequency is the step where we take the tokenized words from the text documents and hash them to a finite feature space. The resulting vectors represent a single document of text. For example, the text 'the brown fox' will hash to a vector of specified length, say 5, such that the result of the hash yields [1,0,2,0,0]. In the case of Spark, the hash used is MurmurHash 3.\n",
    "\n",
    "However, in a large text corpus, some words will be very present (e.g. “the”, “a”, “is”) hence carrying very little meaningful information about the actual contents of the document. If we were to feed the direct count data directly to a classifier those very frequent terms would shadow the frequencies of rarer yet more interesting terms.\n",
    "\n",
    "In order to re-weight the count features into floating point values suitable for usage by a classifier it is very common to incorporate the document frequency of occurrence as a weight or normalization to the term-frequencies mentioned above. Hence, TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfIdf(df, text_col_for_tf_idf, output_tf_idf_col, count_method = 'hash'):\n",
    "    \n",
    "    \"\"\" This fucntion takes the text data and converts it into a term frequency-Inverse Document Frequency vector\n",
    "        The steps for this are tokenization of the input string column, stop word removal, feature hashing/count vectorization depending on \n",
    "        the count_method, and inverse document normalization step.\n",
    "        \n",
    "    Args: \n",
    "        text_col_for_tf_idf: input text column of type 'string' in Java which is used as input to the tokenization, stop word removal and TF-IDF step\n",
    "        output_tf_idf_col: output column to store the resulting feature\n",
    "        count_method: default = 'hash'. Determines whether to use featuer hashing or counts as the TF step for TF-IDF\n",
    "    returns: dataframe with tf-idf vectors\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Carrying out the Tokenization of the text documents (splitting into words)\n",
    "    tokenizer = Tokenizer(inputCol=text_col_for_tf_idf, outputCol=\"tokenised_text\")\n",
    "    tokensDf = tokenizer.transform(df)\n",
    "    # Carrying out the StopWords Removal for TF-IDF\n",
    "    stopwordsremover=StopWordsRemover(inputCol='tokenised_text',outputCol='words')\n",
    "    swremovedDf= stopwordsremover.transform(tokensDf)\n",
    "\n",
    "    if count_method == 'hash':\n",
    "        # hashing is irreversible whereas counting is \n",
    "        # While applying HashingTF only needs a single pass to the data, applying IDF needs two passes:\n",
    "        # First to compute the IDF vector and second to scale the term frequencies by IDF.\n",
    "        hashingTF = HashingTF(inputCol=\"words\", outputCol=\"tf_features\")\n",
    "        tfDf = hashingTF.transform(swremovedDf)\n",
    "    else:\n",
    "        # Creating Term Frequency Vector for each word\n",
    "        cv=CountVectorizer(inputCol=\"words\", outputCol=\"tf_features\", vocabSize=300, minDF=2.0)\n",
    "        cvModel=cv.fit(swremovedDf)\n",
    "        tfDf=cvModel.transform(swremovedDf)\n",
    "\n",
    "    # Carrying out Inverse Document Frequency on the TF data\n",
    "    # spark.mllib's IDF implementation provides an option for ignoring terms\n",
    "    # which occur in less than a minimum number of documents.\n",
    "    # In such cases, the IDF for these terms is set to 0.\n",
    "    # This feature can be used by passing the minDocFreq value to the IDF constructor.\n",
    "    idf=IDF(inputCol=\"tf_features\", outputCol=\"{}\".format(output_tf_idf_col))\n",
    "    idfModel = idf.fit(tfDf)\n",
    "    tfidfDf = idfModel.transform(tfDf)\n",
    "\n",
    "    tfidfDf.cache().count()\n",
    "\n",
    "    return tfidfDf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance Data using Stratified Sampling\n",
    "\n",
    "We do this to ease the memory usage of the TF-IDF. In any case, the data is highly imbalanced, with a current distribution of:\n",
    "\n",
    "- safe: 30333 (~86%)\n",
    "- unsafe: 4136 (~13.2%)\n",
    "- vandal: 270 (~0.8%)\n",
    "\n",
    "It is better to rebalance this by subsampling the \"safe\" class and keeping the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df = get_stratified_sample(df = final_df, fractions = {'safe': 0.15})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "| label|count(1)|\n",
      "+------+--------+\n",
      "|unsafe|     126|\n",
      "|  safe|     102|\n",
      "|vandal|       9|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_label_count(sampled_df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate TF-IDF via Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'Output column tokenised_text already exists.'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\Documents\\University\\KuLeuven\\Semester2\\AA\\assignment3\\AAForB_Assignment3\\spark-2.4.5-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\University\\KuLeuven\\Semester2\\AA\\assignment3\\AAForB_Assignment3\\spark-2.4.5-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o363.transform.\n: java.lang.IllegalArgumentException: Output column tokenised_text already exists.\r\n\tat org.apache.spark.ml.UnaryTransformer.transformSchema(Transformer.scala:112)\r\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\r\n\tat org.apache.spark.ml.UnaryTransformer.transform(Transformer.scala:120)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-f11992bdc5e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtfidfDf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfIdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msampled_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_col_for_tf_idf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"clean_new_text\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_tf_idf_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"new_text_tf_idf_features\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtfidfDf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfIdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidfDf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_col_for_tf_idf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"clean_old_text\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_tf_idf_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"old_text_tf_idf_features\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-586e19b21182>\u001b[0m in \u001b[0;36mtfIdf\u001b[1;34m(df, text_col_for_tf_idf, output_tf_idf_col, count_method)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# Carrying out the Tokenization of the text documents (splitting into words)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext_col_for_tf_idf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"tokenised_text\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mtokensDf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[1;31m# Carrying out the StopWords Removal for TF-IDF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mstopwordsremover\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mStopWordsRemover\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tokenised_text'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'words'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\University\\KuLeuven\\Semester2\\AA\\assignment3\\AAForB_Assignment3\\spark-2.4.5-bin-hadoop2.7\\python\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    171\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\University\\KuLeuven\\Semester2\\AA\\assignment3\\AAForB_Assignment3\\spark-2.4.5-bin-hadoop2.7\\python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\University\\KuLeuven\\Semester2\\AA\\assignment3\\AAForB_Assignment3\\spark-2.4.5-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\University\\KuLeuven\\Semester2\\AA\\assignment3\\AAForB_Assignment3\\spark-2.4.5-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     77\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: 'Output column tokenised_text already exists.'"
     ]
    }
   ],
   "source": [
    "tfidfDf = tfIdf(sampled_df, text_col_for_tf_idf = \"clean_new_text\", output_tf_idf_col = \"new_text_tf_idf_features\")\n",
    "tfidfDf = tfIdf(tfidfDf, text_col_for_tf_idf = \"clean_old_text\", output_tf_idf_col = \"old_text_tf_idf_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram Features on Text Differences (Added/Removed)\n",
    "\n",
    "Here we extract n-gram features from the text differences (text added or removed). The goal is from these simple combinations of words to extract usable features for modelling. Since the words are unordered, an n-gram model is appropriate, as it itself is not necessarily order-preserving in its selection of words.\n",
    "\n",
    "We select $n = 2$ for simplicity of the method. Additionally, we optionally apply feature hashing to the resulting n-grams.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams(df, text_col_for_ngrams, output_col_for_ngrams, n = 2, do_feature_hashing = True):\n",
    "    \"\"\" This fucntion takes a text column and converts it to a (hashed or unhashed) n-gram representation.\n",
    "        The steps are to remove stop words and to run the n-gram, then do optional feature hashing.\n",
    "        \n",
    "    parameter: \n",
    "        text_col_for_ngrams: input text column of typ 'string' in Java which is used as input to the stop word removal and n-gram step\n",
    "        output_col_for_ngrams: output column to store the resulting feature\n",
    "        n: default = 2. Determines the value of n for the n-gram calculation. Example, n = 1 is a unigram of single words.\n",
    "        do_feature_hashing: default = True. Determines whether to use featuer hashing or not\n",
    "    returns: dataframe with n-gram vectors\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ngram = NGram(n=n, inputCol=\"{}\".format(text_col_for_ngrams), outputCol=\"ngrams\")\n",
    "    df = ngram.transform(df)\n",
    "    if do_feature_hashing:\n",
    "        # Carrying out the StopWords Removal for TF-IDF\n",
    "        stopwordsremover=StopWordsRemover(inputCol='ngrams',outputCol='words')\n",
    "        swremovedDf= stopwordsremover.transform(df)\n",
    "        # hashing is irreversible whereas counting is \n",
    "        # While applying HashingTF only needs a single pass to the data, applying IDF needs two passes:\n",
    "        # First to compute the IDF vector and second to scale the term frequencies by IDF.\n",
    "        hashingTF = HashingTF(inputCol=\"words\", outputCol=\"{}\".format(output_col_for_ngrams))\n",
    "        tfDf = hashingTF.transform(swremovedDf)  \n",
    "    return tfDf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfDf = extract_ngrams(tfidfDf, text_col_for_ngrams = \"removed_words\", output_col_for_ngrams = \"removed_words_ngrams_hash_features\")\n",
    "tfidfDf = extract_ngrams(tfidfDf, text_col_for_ngrams = \"added_words\", output_col_for_ngrams = \"added_words_ngrams_hash_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Pipeline and Modelling\n",
    "\n",
    "Below you can find a summary of code needed to extract features using these methods"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "wiki_df = get_wiki_df()\n",
    "\n",
    "clean_df = get_clean_df(wiki_df)\n",
    "\n",
    "# In order to get the actual difference column\n",
    "df_with_difference = get_difference_column(clean_df)\n",
    "\n",
    "final_df = split_difference_into_removed_added(df_with_difference)\n",
    "\n",
    "sampled_df = get_stratified_sample(df = final_df, fractions = {'safe': 0.15})\n",
    "\n",
    "tfidfDf = tfIdf(sampled_df, text_col_for_tf_idf = \"clean_new_text\", output_tf_idf_col = \"new_text_tf_idf_features\")\n",
    "tfidfDf = tfIdf(tfidfDf, text_col_for_tf_idf = \"clean_old_text\", output_tf_idf_col = \"old_text_tf_idf_features\")\n",
    "\n",
    "tfidfDf = extract_ngrams(tfidfDf, text_col_for_ngrams = \"removed_words\", output_col_for_ngrams = \"removed_words_ngrams_hash_features\")\n",
    "tfidfDf = extract_ngrams(tfidfDf, text_col_for_ngrams = \"added_words\", output_col_for_ngrams = \"added_words_ngrams_hash_features\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
