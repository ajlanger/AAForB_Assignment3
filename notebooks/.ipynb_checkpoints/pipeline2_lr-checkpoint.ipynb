{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Feature Engineering and Modelling on Text Data</font> \n",
    "\n",
    "In this notebook, we calculate features on data streamed from seppe.net in Preprocessing.ipynb. We calculate the following features on the data and columns in the extracted wiki_df dataframe:\n",
    "\n",
    "- `TF-IDF`: Term Frequency - Inverse Document Frequency matrix is a feature which measures the occurrence of words normalized by their overall occurrence in the entire document corpus. We use this on the raw edits applied to each Wikipedia article to help gather features as to which words and terms in overall edits may lead to vandal edits or otherwise.\n",
    "- `LDA`: Latent Dirichlet Analysis is a technique used in automated topic discovery. We use this on the overall Wiki text before edit to discover the original topic of the piece. The reason for using this feature is that some topics may be more susceptible to vandalism than others, such as political articles, for example.\n",
    "- `Leichtenstein Distance`: This is used again on the raw edits to quantify the size of the edit. Usually large edits might correspond to large erasures or changes in a document text indicating vandalism and censoring of data from the public."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T14:21:17.947576Z",
     "start_time": "2020-05-09T14:21:16.470734Z"
    }
   },
   "outputs": [],
   "source": [
    "# Perform data transformations\n",
    "from pyspark.sql.functions import lit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine learning libraries\n",
    "from pyspark.ml.feature import HashingTF\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StringIndexer \n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# visualize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# performance evaluation\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics as metric\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# other\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# own functions\n",
    "from functions.performance.metrics import plot_confusion_matrix \n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T14:21:17.954552Z",
     "start_time": "2020-05-09T14:21:17.949564Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bmh', 'classic', 'dark_background', 'fast', 'fivethirtyeight', 'ggplot', 'grayscale', 'seaborn-bright', 'seaborn-colorblind', 'seaborn-dark-palette', 'seaborn-dark', 'seaborn-darkgrid', 'seaborn-deep', 'seaborn-muted', 'seaborn-notebook', 'seaborn-paper', 'seaborn-pastel', 'seaborn-poster', 'seaborn-talk', 'seaborn-ticks', 'seaborn-white', 'seaborn-whitegrid', 'seaborn', 'Solarize_Light2', 'tableau-colorblind10', '_classic_test', '_classic_test_patch']\n"
     ]
    }
   ],
   "source": [
    "# avaialble plotting styles\n",
    "print(plt.style.available)\n",
    "plt.style.use('bmh')\n",
    "\n",
    "# A) ALL DATA\n",
    "LOAD_CLEAN_DATA_PATH = \"../../output/output_data_cleaning.parquet\"\n",
    "\n",
    "# B) SUBSET DATA\n",
    "# LOAD_CLEAN_DATA_PATH = \"../output/output_data_cleaning.parquet\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load output from *preprocessing.ipynb*\n",
    "\n",
    "you can either \n",
    " - **option A**: run `preprocessing.ipynb` and perform the steps below to end up to the `final_df`\n",
    " - **option B** just import the data via: `spark.read.parquet(\"../output/output_preprocessing.parquet\")`\n",
    " \n",
    "**TODO**\n",
    " \n",
    " - Evaluate feature engineering steps on a larget part of the data, the subset dataset containg approx. 800 observations and approx 300 observations after downsampling.\n",
    " - Train `final model` on **all data**.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T14:21:17.963527Z",
     "start_time": "2020-05-09T14:21:17.958541Z"
    }
   },
   "outputs": [],
   "source": [
    "#  %run \"preprocessing.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T14:21:17.973500Z",
     "start_time": "2020-05-09T14:21:17.966520Z"
    }
   },
   "outputs": [],
   "source": [
    "# path = \"../data/subset/*\" (this is the path to the subset data)\n",
    "\n",
    "# wiki_df = get_wiki_df(path=\"../data/subset/*\")\n",
    "\n",
    "# clean_df = get_clean_df(wiki_df)\n",
    "\n",
    "# #In order to get the actual difference column\n",
    "# df_with_difference = get_difference_column(clean_df)\n",
    "# final_df = split_difference_into_removed_added(df_with_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T14:21:17.985469Z",
     "start_time": "2020-05-09T14:21:17.976493Z"
    }
   },
   "outputs": [],
   "source": [
    "# final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T14:21:22.221322Z",
     "start_time": "2020-05-09T14:21:17.988461Z"
    }
   },
   "outputs": [],
   "source": [
    "final_df = spark.read.parquet(LOAD_CLEAN_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Balance data using *stratified sampling*\n",
    "\n",
    "We do this to ease the memory usage of the TF-IDF. In any case, the data is highly imbalanced, with a current distribution of:\n",
    "\n",
    "- safe: 30333 (~86%)\n",
    "- unsafe: 4136 (~13.2%)\n",
    "- vandal: 270 (~0.8%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Split in training and test set\n",
    "\n",
    "preserve balance of classes by performing a **stratisfied split** to get a representive test set\n",
    "\n",
    "**IMPORTANT**\n",
    "- The **training set** is to train the model\n",
    "-  **validation set** is used to find good parameters\n",
    "- The **test set** is to evaluate performance (generalization error) of the final chosen model\n",
    "- When you stream the data: that's model deployment (I don't really consider this as test data), this is actual live incoming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T14:21:48.069600Z",
     "start_time": "2020-05-09T14:21:47.841603Z"
    }
   },
   "outputs": [],
   "source": [
    "# Taking 60% train, 20 % val, 20% test\n",
    "split_ratio_1 = 0.80\n",
    "split_ratio_2 = 0.75 # 0.75 of 0.8  =  0.60\n",
    "\n",
    "train_val = final_df.sampleBy(\"label\",\n",
    "                            fractions={'unsafe': split_ratio_1, 'safe': split_ratio_1, 'vandal': split_ratio_1}, seed=10)\n",
    "\n",
    "# training data\n",
    "train = train_val.sampleBy(\"label\",\n",
    "                            fractions={'unsafe': split_ratio_2, 'safe': split_ratio_2, 'vandal': split_ratio_2}, seed=10)\n",
    "\n",
    "# Subtracting 'train' from 'train_val' to get validation set\n",
    "validaton = train_val.subtract(train)\n",
    "\n",
    "# Subtracting 'train_val' from original 'data' to get test set\n",
    "test = final_df.subtract(train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T14:21:55.744667Z",
     "start_time": "2020-05-09T14:21:48.072593Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "| label|count|\n",
      "+------+-----+\n",
      "|  safe|18121|\n",
      "|unsafe| 2497|\n",
      "|vandal|  162|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T14:22:30.236688Z",
     "start_time": "2020-05-09T14:21:55.747698Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "| label|count|\n",
      "+------+-----+\n",
      "|  safe| 6060|\n",
      "|unsafe|  837|\n",
      "|vandal|   51|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "validaton.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T14:23:03.642684Z",
     "start_time": "2020-05-09T14:22:30.238711Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct pipeline\n",
    "\n",
    " - Spark API pipeline: https://spark.apache.org/docs/latest/ml-pipeline.html (very similar to scikit-learn)\n",
    " - Spark API extact features (e.g. *TF-IDF, N-Gram*): https://spark.apache.org/docs/latest/ml-features.html \n",
    " - Spark API models: https://spark.apache.org/docs/latest/ml-classification-regression.html \n",
    " \n",
    "**TODO**:  \n",
    " - Try out different feature engineering steps and evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Features (new and old text)\n",
    "\n",
    "Term Frequency - Inverse Document Frequency (TF-IDF) is a technique used to build features out of text documents which have theoretically infinite dimensionality without feature reduction techniques such as this. The term-frequency is the step where we take the tokenized words from the text documents and hash them to a finite feature space. The resulting vectors represent a single document of text. For example, the text 'the brown fox' will hash to a vector of specified length, say 5, such that the result of the hash yields [1,0,2,0,0]. In the case of Spark, the hash used is MurmurHash 3.\n",
    "\n",
    "However, in a large text corpus, some words will be very present (e.g. “the”, “a”, “is”) hence carrying very little meaningful information about the actual contents of the document. If we were to feed the direct count data directly to a classifier those very frequent terms would shadow the frequencies of rarer yet more interesting terms.\n",
    "\n",
    "In order to re-weight the count features into floating point values suitable for usage by a classifier it is very common to incorporate the document frequency of occurrence as a weight or normalization to the term-frequencies mentioned above. Hence, TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T14:23:03.654652Z",
     "start_time": "2020-05-09T14:23:03.645677Z"
    }
   },
   "outputs": [],
   "source": [
    "# define stops words\n",
    "# nltk.download('stopwords')\n",
    "STOP_WORDS = list(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *A) clean_new_text*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T14:23:03.858677Z",
     "start_time": "2020-05-09T14:23:03.657152Z"
    }
   },
   "outputs": [],
   "source": [
    "# Carrying out the Tokenization of the text documents (splitting into words)\n",
    "tokenizer_new = Tokenizer(inputCol=\"clean_new_text\",\n",
    "                          outputCol=\"clean_new_tokenised_text\")\n",
    "stopwordsremover_new = StopWordsRemover(\n",
    "    inputCol=tokenizer_new.getOutputCol(), stopWords = STOP_WORDS, outputCol='words_clean_new')\n",
    "\n",
    "# hashing is irreversible whereas counting is\n",
    "hashingTF_new = HashingTF(inputCol=stopwordsremover_new.getOutputCol(\n",
    "), outputCol=\"tf_features_clean_new\", numFeatures=2000)\n",
    "\n",
    "# cv=CountVectorizer(inputCol=\"words\", outputCol=\"tf_features\", vocabSize=300, minDF=2.0)\n",
    "idf_new = IDF(inputCol=hashingTF_new.getOutputCol(),\n",
    "              outputCol=\"feature_clean_new\", minDocFreq=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *B) clean_old_text*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T14:23:03.949683Z",
     "start_time": "2020-05-09T14:23:03.860672Z"
    }
   },
   "outputs": [],
   "source": [
    "# Carrying out the Tokenization of the text documents (splitting into words)\n",
    "tokenizer_old = Tokenizer(inputCol=\"clean_old_text\",\n",
    "                          outputCol=\"clean_old_tokenised_text\")\n",
    "stopwordsremover_old = StopWordsRemover(\n",
    "    inputCol=tokenizer_old.getOutputCol(), stopWords = STOP_WORDS, outputCol='words_clean_old')\n",
    "\n",
    "# hashing is irreversible whereas counting is\n",
    "hashingTF_old = HashingTF(inputCol=stopwordsremover_old.getOutputCol(\n",
    "), outputCol=\"tf_features_clean_old\", numFeatures=2000)\n",
    "\n",
    "# cv=CountVectorizer(inputCol=\"words\", outputCol=\"tf_features\", vocabSize=300, minDF=2.0)\n",
    "idf_old = IDF(inputCol=hashingTF_old.getOutputCol(),\n",
    "              outputCol=\"feature_clean_old\", minDocFreq=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *C) comments*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T14:23:04.037698Z",
     "start_time": "2020-05-09T14:23:03.951677Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer_comment = Tokenizer(inputCol=\"comment\",\n",
    "                              outputCol=\"comment_tokenised_text\")\n",
    "\n",
    "# Carrying out the StopWords Removal for TF-IDF\n",
    "stopwordsremover_comment = StopWordsRemover(\n",
    "    inputCol=tokenizer_comment.getOutputCol(), stopWords=STOP_WORDS, outputCol='words_comment')\n",
    "\n",
    "# hashing is irreversible whereas counting is\n",
    "hashingTF_comment = HashingTF(inputCol=stopwordsremover_comment.getOutputCol(\n",
    "), outputCol=\"tf_features_comment\", numFeatures=2000)\n",
    "\n",
    "# cv=CountVectorizer(inputCol=\"words\", outputCol=\"tf_features\", vocabSize=300, minDF=2.0)\n",
    "idf_comment = IDF(inputCol=hashingTF_comment.getOutputCol(),\n",
    "              outputCol=\"feature_comment\", minDocFreq=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram Features on Text Differences (Added/Removed)\n",
    "\n",
    "Here we extract n-gram features from the text differences (text added or removed). The goal is from these simple combinations of words to extract usable features for modelling. Since the words are unordered, an n-gram model is appropriate, as it itself is not necessarily order-preserving in its selection of words.\n",
    "\n",
    "We select $n = 200$ for simplicity of the method. Additionally, we optionally apply feature hashing to the resulting n-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *A) added_words*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T14:23:04.120701Z",
     "start_time": "2020-05-09T14:23:04.039683Z"
    }
   },
   "outputs": [],
   "source": [
    "ngram_added = NGram(n=200, inputCol=\"added_words\", outputCol=\"ngrams_added\")\n",
    "\n",
    "# Carrying out the StopWords Removal for TF-IDF\n",
    "stopwordsremover_added = StopWordsRemover(\n",
    "    inputCol=ngram_added.getOutputCol(), stopWords = STOP_WORDS, outputCol='words_added')\n",
    "hashingTF_added = HashingTF(inputCol=stopwordsremover_added.getOutputCol(\n",
    "), outputCol=\"feature_added\", numFeatures=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *B) removed_words*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T14:23:04.204702Z",
     "start_time": "2020-05-09T14:23:04.123693Z"
    }
   },
   "outputs": [],
   "source": [
    "ngram_removed = NGram(n=200, inputCol=\"removed_words\",\n",
    "                      outputCol=\"ngrams_removed\")\n",
    "\n",
    "# Carrying out the StopWords Removal for TF-IDF\n",
    "stopwordsremover_removed = StopWordsRemover(\n",
    "    inputCol=ngram_removed.getOutputCol(), stopWords = STOP_WORDS, outputCol='words_removed')\n",
    "hashingTF_removed = HashingTF(inputCol=stopwordsremover_removed.getOutputCol(\n",
    "), outputCol=\"feature_removed\", numFeatures=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Add all steps and define model\n",
    "\n",
    "You can change the model to whathever model you want to try\n",
    "see SPARK API for all models:\n",
    "\n",
    "- Spark API models: https://spark.apache.org/docs/latest/ml-classification-regression.html \n",
    "\n",
    "**TODO**\n",
    "- Explore different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T14:23:04.305711Z",
     "start_time": "2020-05-09T14:23:04.207752Z"
    }
   },
   "outputs": [],
   "source": [
    "# add all features to a vector assembler and call it features (default names for most models)\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"feature_clean_new\", \"feature_clean_old\",\n",
    "               \"feature_comment\",\n",
    "               \"feature_removed\", \"feature_added\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "# make target numeric\n",
    "label_stringIdx = StringIndexer(inputCol=\"label\", outputCol=\"target\")\n",
    "\n",
    "# define model\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001,\n",
    "                        featuresCol='features',\n",
    "                        labelCol='target')\n",
    "\n",
    "# add pipeline steps\n",
    "pipeline = Pipeline(stages=[tokenizer_new, stopwordsremover_new, hashingTF_new, idf_new,\n",
    "                            tokenizer_old, stopwordsremover_old, hashingTF_old, idf_old,\n",
    "                            tokenizer_comment, stopwordsremover_comment, hashingTF_comment,\n",
    "                            ngram_added, stopwordsremover_added, hashingTF_added,\n",
    "                            ngram_removed, stopwordsremover_removed, hashingTF_removed, idf_comment,                       \n",
    "                            assembler, label_stringIdx, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train fit model (pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T14:24:31.326959Z",
     "start_time": "2020-05-09T14:23:04.308730Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit the pipeline to training documents.\n",
    "model_train = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T14:24:33.367755Z",
     "start_time": "2020-05-09T14:24:31.328954Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_train = model_train.transform(train)\n",
    "pred_train.select(\"target\",\"label\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is used for decoding the numeric predictions values back to the original labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T14:24:38.387934Z",
     "start_time": "2020-05-09T14:24:33.424604Z"
    }
   },
   "outputs": [],
   "source": [
    "label_str = list(pred_train.select(\"label\").toPandas()['label'].unique())\n",
    "label_num = [0,1,2]\n",
    "map_num_2_str = dictionary = dict(zip(label_num, label_str))\n",
    "print(map_num_2_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation on training data\n",
    "\n",
    "\n",
    "## A) Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T14:25:08.517088Z",
     "start_time": "2020-05-09T14:24:38.393953Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_names = label_str\n",
    "y_true_train = pred_train.select(\"label\").toPandas()\n",
    "y_pred_train = pred_train.select(\"prediction\").toPandas()\n",
    "y_pred_train = y_pred_train['prediction'].map(map_num_2_str, na_action='ignore')\n",
    "\n",
    "# make confusion matrix\n",
    "cnf_matrix_train = confusion_matrix(y_true=y_true_train, y_pred=y_pred_train, labels=class_names)\n",
    "pd.DataFrame(cnf_matrix_train, columns=class_names, index=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T14:26:26.715861Z",
     "start_time": "2020-05-09T14:26:26.497757Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix_train, classes=label_str, normalize=False)\n",
    "\n",
    "# I need to fix this manually, we should fix this in the future (this look bit clumpsy)\n",
    "plt.yticks([2.5,2,1.5,1,0.5,0,-.5], [\"\",label_str[2],\"\",label_str[1],\"\",label_str[0]]);\n",
    "plt.savefig(\"../output/figures/lr/cm_train.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Summary metrics\n",
    "\n",
    "- Acc\n",
    "- F1\n",
    "- Matthews correlation coefficient\n",
    "- Cohen kappa\n",
    "- AU ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T14:30:39.934078Z",
     "start_time": "2020-05-09T14:30:39.922076Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainingSummary = model_train.stages[20].summary\n",
    "accuracy = trainingSummary.accuracy\n",
    "falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
    "truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
    "fMeasure = trainingSummary.weightedFMeasure()\n",
    "precision = trainingSummary.weightedPrecision\n",
    "recall = trainingSummary.weightedRecall\n",
    "print(\"Accuracy: %s\\nFPR: %s\\nTPR: %s\\nF-measure: %s\\nPrecision: %s\\nRecall: %s\"\n",
    "      % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T14:32:48.973649Z",
     "start_time": "2020-05-09T14:30:39.937118Z"
    }
   },
   "outputs": [],
   "source": [
    "# Acc and F1\n",
    "evaluator_acc = MulticlassClassificationEvaluator(\n",
    "    predictionCol=\"prediction\", labelCol='target', metricName=\"accuracy\")\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "    predictionCol=\"prediction\", labelCol='target', metricName=\"f1\")\n",
    "\n",
    "print(f\"Accuracy: {round(evaluator_acc.evaluate(pred_train), 4)}\")\n",
    "print(f\"F1 score: {round(evaluator_f1.evaluate(pred_train), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T14:32:49.051441Z",
     "start_time": "2020-05-09T14:32:48.977640Z"
    }
   },
   "outputs": [],
   "source": [
    "mcc_train = matthews_corrcoef(y_true_train, y_pred_train)\n",
    "kappa_train = cohen_kappa_score(y_true_train, y_pred_train)\n",
    "\n",
    "print(f\" MCC: {mcc_train}\")\n",
    "print(f\" Kappa: {kappa_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T14:33:18.614175Z",
     "start_time": "2020-05-09T14:32:49.055431Z"
    }
   },
   "outputs": [],
   "source": [
    "# AUC\n",
    "results = pred_train.select(['probability', 'target'])\n",
    "# prepare score-label set\n",
    "results_collect = results.collect()\n",
    "results_list = [(float(i[0][0]), 1.0-float(i[1])) for i in results_collect]\n",
    "scoreAndLabels = sc.parallelize(results_list)\n",
    "\n",
    "metrics = metric(scoreAndLabels)\n",
    "print(\"Area under ROC score is : \", round(metrics.areaUnderROC,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance on validation set\n",
    " - check for reasonable good paramater values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T14:34:16.999070Z",
     "start_time": "2020-05-09T14:33:18.617167Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_val = model_train.transform(validaton)\n",
    "pred_val.select(\"target\",\"label\", \"probability\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T14:35:29.843977Z",
     "start_time": "2020-05-09T14:34:17.002063Z"
    }
   },
   "outputs": [],
   "source": [
    "y_true_val = pred_val.select(\"label\").toPandas()\n",
    "y_pred_val = pred_val.select(\"prediction\").toPandas()\n",
    "y_pred_val = y_pred_val['prediction'].map(map_num_2_str, na_action='ignore')\n",
    "\n",
    "# make confusion matrix\n",
    "cnf_matrix_val = confusion_matrix(y_true=y_true_val, y_pred=y_pred_val, labels=class_names)\n",
    "pd.DataFrame(cnf_matrix_val, columns=class_names, index=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T14:35:30.216515Z",
     "start_time": "2020-05-09T14:35:29.848000Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix_val, classes=label_str, normalize=False, \n",
    "                      title=\"LR\", y_title=1.05)\n",
    "\n",
    "# I need to fix this manually, we should fix this in the future (this look bit clumpsy)\n",
    "plt.yticks([2.5,2,1.5,1,0.5,0,-.5], [\"\",label_str[2],\"\",label_str[1],\"\",label_str[0]]);\n",
    "plt.savefig(\"../output/figures/lr/cm_validation.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Summary Metrics\n",
    "\n",
    "- Acc\n",
    "- F1\n",
    "- Matthews correlation coefficient\n",
    "- Cohen kappa\n",
    "- AU ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T14:37:29.194953Z",
     "start_time": "2020-05-09T14:35:30.224025Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {round(evaluator_acc.evaluate(pred_val), 4)}\")\n",
    "print(f\"F1 score: {round(evaluator_f1.evaluate(pred_val), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T14:37:29.302846Z",
     "start_time": "2020-05-09T14:37:29.200936Z"
    }
   },
   "outputs": [],
   "source": [
    "# mcc and kappa\n",
    "mcc_val = matthews_corrcoef(y_true_val, y_pred_val)\n",
    "kappa_val = cohen_kappa_score(y_true_val, y_pred_val)\n",
    "\n",
    "print(f\" MCC: {mcc_val}\")\n",
    "print(f\" Kappa: {kappa_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T14:38:16.792936Z",
     "start_time": "2020-05-09T14:37:29.358331Z"
    }
   },
   "outputs": [],
   "source": [
    "# AUC\n",
    "results = pred_val.select(['probability', 'target'])\n",
    "# prepare score-label set\n",
    "results_collect = results.collect()\n",
    "results_list = [(float(i[0][0]), 1.0-float(i[1])) for i in results_collect]\n",
    "scoreAndLabels = sc.parallelize(results_list)\n",
    "\n",
    "metrics = metric(scoreAndLabels)\n",
    "print(\"Area under ROC score is : \", round(metrics.areaUnderROC,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance of final model (pipeline) on test set\n",
    "\n",
    "\n",
    "**Important**\n",
    " - Only use this dataset once you decided on your final model.\n",
    " - This is only to get and idea of your models performance in real live (when we start streaming)\n",
    " - !! Do never make any decision on the test data, only on the validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First fit model on both training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T14:39:33.216990Z",
     "start_time": "2020-05-09T14:38:16.795929Z"
    }
   },
   "outputs": [],
   "source": [
    "model_train_val = pipeline.fit(train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T14:40:15.755604Z",
     "start_time": "2020-05-09T14:39:33.219982Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_test = model_train_val.transform(test)\n",
    "pred_test.select(\"target\",\"label\", \"probability\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T14:41:10.915084Z",
     "start_time": "2020-05-09T14:40:15.758595Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class_names = ['unsafe','safe', 'vandal']\n",
    "y_true_test = pred_test.select(\"label\").toPandas()\n",
    "y_pred_test = pred_test.select(\"prediction\").toPandas()\n",
    "y_pred_test = y_pred_test['prediction'].map(map_num_2_str, na_action='ignore')\n",
    "\n",
    "# make confusion matrix\n",
    "cnf_matrix_test = confusion_matrix(y_true=y_true_test, y_pred=y_pred_test, labels=class_names)\n",
    "pd.DataFrame(cnf_matrix_test, columns=class_names, index=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T14:41:11.246198Z",
     "start_time": "2020-05-09T14:41:10.919075Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix_test, classes=label_str, normalize=False)\n",
    "\n",
    "# I need to fix this manually, we should fix this in the future (this look bit clumpsy)\n",
    "plt.yticks([2.5,2,1.5,1,0.5,0,-.5], [\"\",label_str[2],\"\",label_str[1],\"\",label_str[0]]);\n",
    "plt.savefig(\"../output/figures/lr/cm_test.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Summary Metrics\n",
    "\n",
    "- F1\n",
    "- Matthews correlation coefficient\n",
    "- Cohen kappa\n",
    "- AU ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T14:42:31.932778Z",
     "start_time": "2020-05-09T14:41:11.249191Z"
    }
   },
   "outputs": [],
   "source": [
    "# Acc and F1\n",
    "print(f\"Accuracy: {round(evaluator_acc.evaluate(pred_test), 4)}\")\n",
    "print(f\"F1 score: {round(evaluator_f1.evaluate(pred_test), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T14:42:31.977659Z",
     "start_time": "2020-05-09T14:42:31.935771Z"
    }
   },
   "outputs": [],
   "source": [
    "mcc_test = matthews_corrcoef(y_true_test, y_pred_test)\n",
    "kappa_test = cohen_kappa_score(y_true_test, y_pred_test)\n",
    "\n",
    "print(f\" MCC: {mcc_test}\")\n",
    "print(f\" Kappa: {kappa_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T14:43:02.424247Z",
     "start_time": "2020-05-09T14:42:31.980651Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# AUC\n",
    "results = pred_test.select(['probability', 'target'])\n",
    "# prepare score-label set\n",
    "results_collect = results.collect()\n",
    "results_list = [(float(i[0][0]), 1.0-float(i[1])) for i in results_collect]\n",
    "scoreAndLabels = sc.parallelize(results_list)\n",
    "\n",
    "metrics = metric(scoreAndLabels)\n",
    "print(\"Area under ROC score is : \", round(metrics.areaUnderROC,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train final model (pipline) on all data and save\n",
    "\n",
    "**IMPORTANT**\n",
    "\n",
    "- train your final model on **ALL DATA** using the parameters found above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T14:44:34.228605Z",
     "start_time": "2020-05-09T14:43:02.426242Z"
    }
   },
   "outputs": [],
   "source": [
    "final_model = pipeline.fit(sampled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T14:44:47.907600Z",
     "start_time": "2020-05-09T14:44:34.231598Z"
    }
   },
   "outputs": [],
   "source": [
    "final_model.write().overwrite().save(\"../output/models/logistic_regression\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
