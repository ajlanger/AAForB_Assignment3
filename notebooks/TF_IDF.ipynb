{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering on Text Data\n",
    "\n",
    "In this notebook, we calculate features on data streamed from seppe.net in Preprocessing.ipynb. We calculate the following features on the data and columns in the extracted wiki_df dataframe:\n",
    "\n",
    "- TF-IDF: Term Frequency - Inverse Document Frequency matrix is a feature which measures the occurrence of words normalized by their overall occurrence in the entire document corpus. We use this on the raw edits applied to each Wikipedia article to help gather features as to which words and terms in overall edits may lead to vandal edits or otherwise.\n",
    "- LDA: Latent Dirichlet Analysis is a technique used in automated topic discovery. We use this on the overall Wiki text before edit to discover the original topic of the piece. The reason for using this feature is that some topics may be more susceptible to vandalism than others, such as political articles, for example.\n",
    "- Leichtenstein Distance: This is used again on the raw edits to quantify the size of the edit. Usually large edits might correspond to large erasures or changes in a document text indicating vandalism and censoring of data from the public."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the feature transformation classes for doing TF-IDF \n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover, CountVectorizer, IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run \"preprocessing.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "| label|count(1)|\n",
      "+------+--------+\n",
      "|  safe|   16567|\n",
      "|unsafe|    2291|\n",
      "|vandal|     154|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wiki_df = get_wiki_df()\n",
    "\n",
    "get_label_count(wiki_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|             comment|label|         name_user|            text_new|            text_old|          title_page|            url_page|\n",
      "+--------------------+-----+------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|→‎4 February:adde...| safe|SebastianRueckoldt|{{see also|Timeli...|{{see also|Timeli...|Timeline of the 2...|//en.wikipedia.or...|\n",
      "|removing duplicat...| safe|Andreas Philopater|{{short descripti...|{{short descripti...|List of Art Deco ...|//en.wikipedia.or...|\n",
      "+--------------------+-----+------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wiki_df.show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19012, 7)\n",
      "['label', 'comment', 'title_page', 'name_user', 'text_old', 'text_new', 'difference']\n"
     ]
    }
   ],
   "source": [
    "# Get clean dataframe (cleaning of comment, title_page, name_user):\n",
    "clean_df = get_clean_df(wiki_df)\n",
    "\n",
    "# In order to get the actual difference column\n",
    "df_with_difference = get_difference_column(clean_df)\n",
    "\n",
    "# Example of a difference column of the first of 20 instances:\n",
    "# difference column is in the form : {removed: [...], added: [...]} in order to know which words were added and which were removed\n",
    "print((df_with_difference.count(), len(df_with_difference.columns)))\n",
    "print(df_with_difference.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfIdf(df, count_method = 'hash'):\n",
    "    \"\"\" This fucntion takes the text data and converts it into a term frequency-Inverse Document Frequency vector\n",
    "\n",
    "    parameter: \n",
    "        count_method: Default = 'hash'. Determines whether to use featuer hashing or counts as the TF step for TF-IDF\n",
    "    returns: dataframe with tf-idf vectors\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Carrying out the Tokenization of the text documents (splitting into words)\n",
    "    tokenizer = Tokenizer(inputCol=\"text_new\", outputCol=\"tokenised_text\")\n",
    "    tokensDf = tokenizer.transform(df)\n",
    "    # Carrying out the StopWords Removal for TF-IDF\n",
    "    stopwordsremover=StopWordsRemover(inputCol='tokenised_text',outputCol='words')\n",
    "    swremovedDf= stopwordsremover.transform(tokensDf)\n",
    "\n",
    "    if count_method == 'hash':\n",
    "        # hashing is irreversible whereas counting is \n",
    "        # While applying HashingTF only needs a single pass to the data, applying IDF needs two passes:\n",
    "        # First to compute the IDF vector and second to scale the term frequencies by IDF.\n",
    "        hashingTF = HashingTF(inputCol=\"words\", outputCol=\"tf_features\")\n",
    "        tfDf = hashingTF.transform(swremovedDf)\n",
    "    else:\n",
    "        # Creating Term Frequency Vector for each word\n",
    "        cv=CountVectorizer(inputCol=\"words\", outputCol=\"tf_features\", vocabSize=300, minDF=2.0)\n",
    "        cvModel=cv.fit(swremovedDf)\n",
    "        tfDf=cvModel.transform(swremovedDf)\n",
    "\n",
    "    # Carrying out Inverse Document Frequency on the TF data\n",
    "    # spark.mllib's IDF implementation provides an option for ignoring terms\n",
    "    # which occur in less than a minimum number of documents.\n",
    "    # In such cases, the IDF for these terms is set to 0.\n",
    "    # This feature can be used by passing the minDocFreq value to the IDF constructor.\n",
    "    idf=IDF(inputCol=\"tf_features\", outputCol=\"tf_idf_features\")\n",
    "    idfModel = idf.fit(tfDf)\n",
    "    tfidfDf = idfModel.transform(tfDf)\n",
    "\n",
    "    tfidfDf.cache().count()\n",
    "\n",
    "    return tfidfDf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'safe': 0.8, 'unsafe': 0.8, 'vandal': 0.8}\n",
      "+--------------------+------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|             comment| label|           name_user|            text_new|            text_old|          title_page|            url_page|\n",
      "+--------------------+------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|→‎General strike ...|  safe|  Twofingered Typist|{{short descripti...|{{short descripti...|2019–20 Hong Kong...|//en.wikipedia.or...|\n",
      "|→‎International r...|  safe|             CRau080|{{short descripti...|{{short descripti...|2019–20 Hong Kong...|//en.wikipedia.or...|\n",
      "|                    |  safe|           Promiseus|{{short descripti...|{{short descripti...|2019–20 Hong Kong...|//en.wikipedia.or...|\n",
      "|       →‎December:gr|  safe|              TVSGuy|{{USTV year|2019}...|{{USTV year|2019}...|2019 in American ...|//en.wikipedia.or...|\n",
      "|→‎Italy:tense, be...|  safe|         Elmer Clark|{{pp-protected|sm...|{{pp-protected|sm...|2019–20 coronavir...|//en.wikipedia.or...|\n",
      "|                    |  safe|          King Zowie|{{pp-protected|sm...|{{pp-protected|sm...|2019–20 coronavir...|//en.wikipedia.or...|\n",
      "|            (→‎1919)|unsafe|       96.40.153.184|{{short descripti...|{{short descripti...|List of accidents...|//en.wikipedia.or...|\n",
      "|→‎100 million to ...|  safe|             Politsi|{{short descripti...|{{short descripti...|List of best-sell...|//en.wikipedia.or...|\n",
      "|            (→‎Plot)|unsafe|    Crossley Osborne|{{about|the 2013 ...|{{about|the 2013 ...|  Frozen (2013 film)|//en.wikipedia.or...|\n",
      "|                    |unsafe|        Annette.spry|{{pp-protected|sm...|{{pp-protected|sm...|                2019|//en.wikipedia.or...|\n",
      "|       (Description)|unsafe|2a00:23c4:df9c:29...|{{pp-move-indef}}...|{{pp-move-indef}}...|   Pope John Paul II|//en.wikipedia.or...|\n",
      "|                    |unsafe|       66.44.105.123|{{Use mdy dates|d...|{{Use mdy dates|d...|Native Americans ...|//en.wikipedia.or...|\n",
      "|→‎Local pandemic ...|  safe|       Night Lantern|{{short descripti...|{{short descripti...|2020 coronavirus ...|//en.wikipedia.or...|\n",
      "|            wikilink|  safe|            Melofors|{{short descripti...|{{short descripti...|List of accidents...|//en.wikipedia.or...|\n",
      "|                    |  safe|           Bbbzzziii|{{short descripti...|{{short descripti...|2020 coronavirus ...|//en.wikipedia.or...|\n",
      "|→‎Kuala Lumpur re...|  safe|       Night Lantern|{{short descripti...|{{short descripti...|2020 coronavirus ...|//en.wikipedia.or...|\n",
      "|Disambiguated:Kat...|  safe|                Rodw|{{For|related rac...|{{For|related rac...|2020 United State...|//en.wikipedia.or...|\n",
      "|(→‎References:rem...|unsafe|        81.106.2.251|{{short descripti...|{{short descripti...|    Sebastian Vettel|//en.wikipedia.or...|\n",
      "|→‎Public transpor...|  safe|             Wire723|{{pp-protected|sm...|{{pp-protected|sm...|2020 coronavirus ...|//en.wikipedia.or...|\n",
      "|                    |unsafe|       24.197.102.15|{{short descripti...|{{short descripti...|         A.J. Styles|//en.wikipedia.or...|\n",
      "+--------------------+------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "fractions = wiki_df.select(\"label\").distinct().withColumn(\"fraction\", lit(0.8)).rdd.collectAsMap()\n",
    "print(fractions) \n",
    "fractions = {'safe': 0.1, 'unsafe': 1.0, 'vandal':1.0}\n",
    "\n",
    "seed = 42\n",
    "# {2147481832: 0.8, 214748183: 0.8}\n",
    "sampled_df = wiki_df.stat.sampleBy(\"label\", fractions, seed)\n",
    "sampled_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "| label|count(1)|\n",
      "+------+--------+\n",
      "|  safe|    1680|\n",
      "|unsafe|    2291|\n",
      "|vandal|     154|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_label_count(sampled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate TF-IDF via Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfDf=tfIdf(sampled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|     tf_idf_features|\n",
      "+--------------------+\n",
      "|(262144,[14,115,1...|\n",
      "|(262144,[14,115,1...|\n",
      "|(262144,[14,115,1...|\n",
      "|(262144,[4,15,29,...|\n",
      "|(262144,[8,14,60,...|\n",
      "|(262144,[8,14,60,...|\n",
      "|(262144,[11,120,2...|\n",
      "|(262144,[14,90,13...|\n",
      "|(262144,[13,14,15...|\n",
      "|(262144,[3,15,62,...|\n",
      "|(262144,[14,15,83...|\n",
      "|(262144,[14,20,30...|\n",
      "|(262144,[112,211,...|\n",
      "|(262144,[11,15,22...|\n",
      "|(262144,[14,67,97...|\n",
      "|(262144,[112,211,...|\n",
      "|(262144,[15,24,13...|\n",
      "|(262144,[36,41,21...|\n",
      "|(262144,[14,127,1...|\n",
      "|(262144,[14,115,1...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidfDf.select(\"tf_idf_features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            text_new|\n",
      "+--------------------+\n",
      "|{{short descripti...|\n",
      "|{{short descripti...|\n",
      "|{{short descripti...|\n",
      "|{{USTV year|2019}...|\n",
      "|{{pp-protected|sm...|\n",
      "|{{pp-protected|sm...|\n",
      "|{{short descripti...|\n",
      "|{{short descripti...|\n",
      "|{{about|the 2013 ...|\n",
      "|{{pp-protected|sm...|\n",
      "|{{pp-move-indef}}...|\n",
      "|{{Use mdy dates|d...|\n",
      "|{{short descripti...|\n",
      "|{{short descripti...|\n",
      "|{{short descripti...|\n",
      "|{{short descripti...|\n",
      "|{{For|related rac...|\n",
      "|{{short descripti...|\n",
      "|{{pp-protected|sm...|\n",
      "|{{short descripti...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidfDf.select(\"text_new\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
